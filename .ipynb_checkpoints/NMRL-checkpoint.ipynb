{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/JacobFV/AI-Resources/blob/master/NMRL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THIS IS JUST BRAINSTORMING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": "true",
    "id": "R3osJDsOWh9x",
    "tags": []
   },
   "source": [
    "### Copyright &copy; 2020 Jacob Valdez\n",
    "\n",
    "under the [MIT License](https://opensource.org/licenses/MIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JRt5qad9FhW5"
   },
   "source": [
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8B9PDrjc7ZOc"
   },
   "source": [
    "# Nodal Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sFA20B_2WhLi"
   },
   "source": [
    "Abstract and pictures/videos *en simulo*. Maybe also simplified diagrams of AGI node and network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EfzF3eBEHqsZ"
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kxImb7wSHuhQ"
   },
   "source": [
    "AGI-0 rests on the hypotheses that:\n",
    "- encoding the principles of [free energy minimization](https://en.wikipedia.org/wiki/Free_energy_principle) in a multilayer hidden Markov chain prediction unit will enable unsupervised, open-ended learning\n",
    "- competitive ensembles of prediction units will succeed in overcoming the [no free lunch theorom](https://en.wikipedia.org/wiki/No_free_lunch_theorem)\n",
    "- ~~supervised multiagent simulations will enable learning mirroring features that transfer successfully to humans~~\n",
    "- embedded natural language self examination objectives will automatically align agent behavior with encoded human values from environment reinforcement without any supervised reward administration\n",
    "- **the realization of these hypotheses will achieve artificial general intelligence**\n",
    "\n",
    "Discuss my theory of hierarchial conditioning here\n",
    "\n",
    "Each child only provides additional conditioning context but is not required for a module to produce an output. (Kind of like how [conditioning gpt-2 with a longer input sentence gives you more control over its output](https://colab.research.google.com/drive/12_EzHGVE8ZnJ-MA9W_8tQuP6xHpWb1X3?usp=sharing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pB2r4zDE77_X"
   },
   "source": [
    "# Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JOgsBzshrVza"
   },
   "source": [
    "$\\newcommand{\\ab}{\\mathcal{A}}$\n",
    "$\\newcommand{\\pr}{\\mathcal{P}}$\n",
    "$\\newcommand{\\pol}{\\mathcal{\\pi}}$\n",
    "$\\newcommand{\\o}[1][t]{{o}_{#1}}$\n",
    "$\\newcommand{\\otrue}[1][t]{{o}_{#1}^{true}}$\n",
    "$\\newcommand{\\opast}[1][t-T_o:t]{{o}_{#1}}$\n",
    "$\\newcommand{\\otruepast}[1][t-T_o:t]{{o}_{#1}^{true}}$\n",
    "$\\newcommand{\\s}[1][t]{{s}_{#1}}$\n",
    "$\\newcommand{\\spred}[1][t+1]{{s}_{#1}^{pred}}$\n",
    "$\\newcommand{\\oimagpast}[1][\\tau-T_o:\\tau]{{o}_{#1}^{imag}}$\n",
    "$\\newcommand{\\oimag}[1][\\tau]{{o}_{#1}^{imag}}$\n",
    "$\\newcommand{\\simag}[1][\\tau]{{s}_{#1}^{imag}}$\n",
    "$\\newcommand{\\spredimag}[1][\\tau+p]{{s}_{#1}^{pred,imag}}$\n",
    "$\\newcommand{\\stargetimag}[1][\\tau+p]{{s}_{#1}^{*,imag}}$\n",
    "$\\newcommand{\\starget}[1][t+1]{{s}_{#1}^*}$\n",
    "$\\newcommand{\\otarget}[1][t+1]{{o}_{#1}^*}$\n",
    "$\\newcommand{\\sblind}[1][\\tau+p]{{s}_{#1}^{blind}}$\n",
    "$\\newcommand{\\pe}[1][t]{pe_{#1}}$\n",
    "$\\newcommand{\\n}[1][i]{n_{#1}}$\n",
    "$\\newcommand{\\nparent}{\\mathcal{N_{pa}}}$\n",
    "$\\newcommand{\\nchildren}{\\mathcal{N_{ch}}}$\n",
    "$\\newcommand{\\U}{\\mathcal{U}}$\n",
    "$\\newcommand{\\Ex}[1][ ]{ \\underset{#1}{ \\mathbb{E} } }$\n",
    "$ \\newcommand{\\vect}[1]{\\boldsymbol{#1}} $\n",
    "$\\def\\falert{f_{alert}}$\n",
    "$\\def\\NN{N\\kern-0.2em N}$\n",
    "$\\def\\DS{\\mathcal{D}}$\n",
    "$\\def\\IE{\\mathcal{E}_I}$\n",
    "$\\def\\CER{\\mathcal{C}}$\n",
    "$\\newcommand{norm}[1]{\\bigl\\lvert\\bigl\\rvert{#1}\\bigr\\lvert\\bigr\\rvert}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9xvi2Xsw09co"
   },
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xaoa_JQAvWy8"
   },
   "source": [
    "Nodal-RL operates by minimizing predictive entropy. It does this by decomposing the state space with a [structured probabblistic model](https://en.wikipedia.org/wiki/Graphical_model) into sensory, information, and actuator nodes. Each *information node* on the graph only models a subset of all information. The directed and potentially cyclic model $\\langle\\mathcal{N}\\kern-0.15em,\\mathcal{E}\\rangle$ may look like:\n",
    "\n",
    "![example network](https://mermaid.ink/img/eyJjb2RlIjoiZ3JhcGggTFJcbiAgUzAwW3BpeGVsc10gLS0-IFMwMSh2aXN1YWwgZmVhdHVyZXMpXG4gIFMwMSAtLT4gUDAoY29tcG9zaXRlIGZlYXR1cmVzKVxuXG4gIFMxMFthdWRpbyBzcGVjdHJvZ3JhcGhdIC0tPiBTMTEodG9rZW5zKVxuICBTMTEgLS0-IFAwKGNvbXBvc2l0ZSBmZWF0dXJlcylcblxuICBTMDEgLS0-IFNYMDEwXG4gIFMxMSAtLT4gU1gwMTBcbiAgU1gwMTAobmF0dXJhbCBsYW5ndWFnZSBJKSAtLT4gU1gwMTFcbiAgU1gwMTEobmF0dXJhbCBsYW5ndWFnZSBJSSkgLS0-IFAwXG5cbiAgUzIwW2FjdHVhdG9yIHNpZ25hbHNdIC0tPiBTMjEobW90aW9uIHByaW1pdGl2ZXMpXG4gIFMyMSAtLT4gUDAoY29tcG9zaXRlIGZlYXR1cmVzKVxuXG4gIFAwIC0tPiBQMDAoYWJzdHJhY3QgcGVyc3BlY3RpdmUgSSlcbiAgUDAgLS0-IFAwMShhYnN0cmFjdCBwZXJzcGVjdGl2ZSBJSSlcbiAgUDAgLS0-IFAwMihhYnN0cmFjdCBwZXJzcGVjdGl2ZSBJSUkpXG5cdFxuICBQMDAgLS0-IFAwMDAoYWJzdHJhY3QgcGVyc3BlY3RpdmUgSVYpXG4gIFAwMCAtLT4gUDAwMShhYnN0cmFjdCBwZXJzcGVjdGl2ZSBWKVxuICBQMDIgLS0-IFAwMDFcblxuICBQMDEgLS0-IFAwMDBcblxuICBQMDAwIC0tPiBQMCIsIm1lcm1haWQiOnsidGhlbWUiOiJkZWZhdWx0In0sInVwZGF0ZUVkaXRvciI6ZmFsc2V9) <!-- https://mermaid-js.github.io/mermaid-live-editor/#/edit/eyJjb2RlIjoiZ3JhcGggTFJcbiAgUzAwW3BpeGVsc10gLS0-IFMwMSh2aXN1YWwgZmVhdHVyZXMpXG4gIFMwMSAtLT4gUDAoY29tcG9zaXRlIGZlYXR1cmVzKVxuXG4gIFMxMFthdWRpbyBzcGVjdHJvZ3JhcGhdIC0tPiBTMTEodG9rZW5zKVxuICBTMTEgLS0-IFAwKGNvbXBvc2l0ZSBmZWF0dXJlcylcblxuICBTMDEgLS0-IFNYMDEwXG4gIFMxMSAtLT4gU1gwMTBcbiAgU1gwMTAobmF0dXJhbCBsYW5ndWFnZSBJKSAtLT4gU1gwMTFcbiAgU1gwMTEobmF0dXJhbCBsYW5ndWFnZSBJSSkgLS0-IFAwXG5cbiAgUzIwW2FjdHVhdG9yIHNpZ25hbHNdIC0tPiBTMjEobW90aW9uIHByaW1pdGl2ZXMpXG4gIFMyMSAtLT4gUDAoY29tcG9zaXRlIGZlYXR1cmVzKVxuXG4gIFAwIC0tPiBQMDAoYWJzdHJhY3QgcGVyc3BlY3RpdmUgSSlcbiAgUDAgLS0-IFAwMShhYnN0cmFjdCBwZXJzcGVjdGl2ZSBJSSlcbiAgUDAgLS0-IFAwMihhYnN0cmFjdCBwZXJzcGVjdGl2ZSBJSUkpXG5cdFxuICBQMDAgLS0-IFAwMDAoYWJzdHJhY3QgcGVyc3BlY3RpdmUgSVYpXG4gIFAwMCAtLT4gUDAwMShhYnN0cmFjdCBwZXJzcGVjdGl2ZSBWKVxuICBQMDIgLS0-IFAwMDFcblxuICBQMDEgLS0-IFAwMDBcblxuICBQMDAwIC0tPiBQMCIsIm1lcm1haWQiOnsidGhlbWUiOiJkZWZhdWx0In0sInVwZGF0ZUVkaXRvciI6ZmFsc2V9 -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hzy5vvXwfdLj"
   },
   "source": [
    "Above, among many other partial models, is modeled the information that $\\n[tokens]$ and $\\n[visual\\ features]$ supply to $\\n[nat\\ lang\\ I]$, the information $\\n[nat\\ lang\\ I]$ supplies to $\\n[nat\\ lang\\ II]$, and the partial information $\\n[nat\\ lang\\ II]$ adds to a composite understanding of the world. This composite understanding is captured in the state variable $\\n[comp].\\s[ ]$. Instead of producing its state from directly observing sensory data, $\\n[comp]$ only observes its parent nodes' states $\\n[comp].\\o[ ]=\\{\\n[nat\\ lang\\ II].\\s[ ],\\n[motion\\ prim].\\s[ ],\\n[visual\\ features].\\s[ ]\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kQn8w1madzaF"
   },
   "source": [
    "An information node $\\n$ attempts to build an internal model of its observed variables $P(\\o[t+p]|\\opast)$. It does this by first *abstracting* the information from its observable data trajectory into an internal *state* $\\s=\\ab(\\opast)$. It then attempts to model the trajectory of that internal state with a *predictor* function $\\spred=\\pr(\\s)$. Predictive error $D_{KL}(\\spred,\\s[t+1])$ and predictive uncertainty $H(\\spred)$ then become the objectives to cumulatively minimize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "afZf7OVMpINQ"
   },
   "source": [
    "The *policy* $\\pol$ specfies *target observations* $\\otarget$ that approach these objectives. Target observations belonging to a child layer influence their parent's *target state* (e.g.: $\\n[nat\\ lang\\ II].\\otarget \\stackrel{affects}{\\rightarrow} \\n[nat\\ lang\\ I].\\starget$). Since a node may have multiple children, the target state is detirmined by a $\\n[ch].w_{\\mathcal{Pa}_i}$ weighted mean of the node children's target observations $\\{\\n[ch].\\otarget\\ |\\ \\forall\\n[ch]\\in\\n\\smash{.}\\nchildren\\}$ concatenated with the node's own predicted state $\\n\\smash{.}\\spred$. Additionally, proabblistic certainty weighting, ($\\CER(x)=0=$ very indecisive) weakens the influence of vague target distributions.\n",
    "\n",
    "$\\text{weighted ave}(\\{\\langle w_{pred},\\spred\\rangle\\}\\cup\\{\\langle \\n[ch].w_{\\mathcal{Pa}_i}\\CER(\\n[ch].\\otarget), \\n[ch].\\otarget\\rangle\\ |\\ \\forall\\n[ch]\\in\\nchildren\\})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HinI6og1pCM5"
   },
   "source": [
    "Nodes do not compute updates simultaneously or even at the same frequency though. They are individually updated at periods of $\\n\\smash{.}p=\\n\\smash{.}f^{-1}$. As a result, $\\n[ch].\\otarget[t+\\n\\smash{.}p]$ may not exist. To avoid blocking, child node target observations are instead queried for the most recent existing record $\\n[ch].\\otarget[\\tau]\\ |\\ \\exists\\n[ch].\\otarget[\\tau];\\ \\tau=\\max [t-N_o:t+p];$ An exponential recency decay $e^{-\\n[ch].\\lambda(t+p-\\tau)}$ additionally weights the target observation. The target state is thus:\n",
    "\n",
    "$\\starget[t+p]=\\text{weighted ave}(\\{\\langle w_{pred},\\spred[t+p]\\rangle\\} \\cup \\{\\langle \\n[ch].w_{\\mathcal{Pa}_i}\\CER(\\n[ch].\\otarget[\\tau])\\times e^{-\\n[ch].\\lambda(t+p-\\tau)}, \\n[ch].\\otarget[\\tau]\\rangle\\ |\\ \\exists\\n[ch].\\otarget[\\tau];\\ \\tau=\\max [t-N_o:t+p];\\ \\forall\\n[ch]\\in\\nchildren\\})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xFBEg1oBpD5p"
   },
   "source": [
    "Target state in turn biases the target observation via the policy $\\otarget=\\pol(\\s,\\starget)$. Since the target state may be a weighted mean of several stochastic variables, child target observations with a high entropy will have less influence in the weighted mean distribution-even if its weight is large. The more certain (lower entropy) a target observation is, the greator influence it will have on its parent target state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AyH01fgTtToo"
   },
   "source": [
    "This makes sibling nodes form an implicit team. One node $\\n[abs\\ per\\ I]$ may have a very clear understanding of the dynamics of one subspace of the observation space $\\mathcal{O}_i\\subset\\mathcal{O}$, hence producing low entropy predictions and strong opinions on what $\\otarget[t+p]$ should be. At the same time, its sibling $\\n[abs\\ per\\ II]$ may not have knowledge of that domain and therefore produce highly irrational, entropic observation targets. The result of averaging the two nodes' target observations is a distribution with relatively little influence by $\\n[abs\\ per\\ II]$. Togethor, nodes form a network of experts each specializing in unique domains to give the agent open-ended intelligence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n-cwPGm9DZSO"
   },
   "source": [
    "## Stochastic Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g6ttdPHyWEmc"
   },
   "source": [
    "To allow information nodes to produce multimodal and multivariate observation and state distributions, the following reparametrization trick is employed to layer input concatenated with a multidimensional uniform distribution:\n",
    "\n",
    "$\\vect{y}=\\sigma(W[\\vect{x};\\vect{\\zeta}]+b)\\qquad\\vect{\\zeta}\\thicksim\\{\\mathcal{U(0,1)}\\}^{N_\\zeta}$\n",
    "<!-- $y=\\sigma(\\zeta\\vect{w}_\\sigma\\cdot\\vect{x}+\\vect{w}_\\mu\\cdot\\vect{x}+b) \\qquad \\zeta\\thicksim\\mathcal{U}(0,1)$\n",
    "\n",
    "or for a dense layer:\n",
    "\n",
    "$\\vect{y}=\\sigma(\\vect{\\zeta}W_\\sigma\\vect{x}+\\vect{w}_\\mu\\cdot\\vect{x}+b) \\qquad \\zeta\\thicksim\\mathcal{U}(0,1)$ -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o3oYTvHderF9"
   },
   "source": [
    "Since this design employs continuous distributions, certainty $\\CER$ is measured by the relative entropy of the uniform distribution with respect to a target distribution $P$:\n",
    "\n",
    "$\\CER(P)=-D_{KL}(\\mathcal{U}(a,b)||P)$\n",
    "\n",
    "This above definition demands a definition of activation bounds $a$ and $b$ for each distribution. These are regularized distribution-wise by:\n",
    "\n",
    "$\\loss_{bounds}(x)=\\begin{cases}f(a-x)\\qquad&x\\lt a\\\\0\\qquad& a\\le x\\le b\\\\f(x-b)\\qquad& x\\gt b\\end{cases}$\n",
    "\n",
    "with $f(x)=\\{x,x^2,x^n,e^x,\\dots\\}$ being any differentiable monotonically increasing function on $(0,\\infty)$.\n",
    "\n",
    "The certainty of various distributions zero as they are more unpredictable and increases to above one as they become more certain of the likelihood of a random variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q3eej_jcg5sg"
   },
   "source": [
    "## Dreaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eMSVqV6Shep3"
   },
   "source": [
    "Not all observations collected online are equally relevant to future tasks. Additionally, when observations add little information to an already highly unpredictable state trajectory, it becomes pointless to informatively utilize the next state prediction. In these situations, informations nodes maintain productivity by *dreaming*.\n",
    "\n",
    "Dreaming makes a shift from closed to open loop control. It involves relying less on observed data and more on anticipated observations. It mixes the true observation $\\otrue$ with last timestep's target observation $\\otarget[t]$ by a nonlinear function $\\falert$ to produce the observed processed by all other functions:\n",
    "\n",
    "$\\o=\\falert\\otrue+(1-\\falert)\\otarget[t]$\n",
    "\n",
    "where $\\falert$ increases alertness when surprising, unexpected, or informative observations occur $\\falert\\propto I(\\otrue)$ and decreases alertness when predictability is low $\\falert\\propto (H(\\s[t-p]))^{-1}$. To ensure the transition to dreaming and back to alertness is not jittery, these factors are smoothed with a rolling average and transformed with $\\tanh$:\n",
    "\n",
    "$\\overline{\\underline{\\textbf{Algorithm:}\\ \\text{Percieved Observation }\\o}}$\n",
    "\n",
    "$1.\\ d_t^{ave}=d_t^{ave}+\\alpha_{d,1}\\frac{I(\\otrue)}{H(\\s[t-p])}$\n",
    "\n",
    "$2.\\ \\falert=\\tanh[\\alpha_{d,2}(d_t^{ave}-\\alpha_{d,3})]$\n",
    "\n",
    "$3.\\ \\mathbf{return}\\ \\o=\\falert\\otrue+(1-\\falert)\\otarget[t]$\n",
    "\n",
    "where $\\alpha_{d,1}\\lt1$ is small number that influences the rolling average latency, $\\alpha_{d,2}\\gt1$ is large and makes the transition from dream to alert state abrupt, and $\\alpha_{d,3}$ is the minimum average ratio of information to entropy that must be provided by observations for $\\n$ to pay attention to them.\n",
    "\n",
    "Since dreamed state trajectories will generally have a larger entropy than states based on true observations, dreaming node target observations have a correspondingly high entropy and generally do not compete with target observations from sibling nodes that are not dreaming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bQvkTUhBy9d2"
   },
   "source": [
    "## Running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aHAkOGZCg1JC"
   },
   "source": [
    "Information nodes join an asynchronously updating netowrk with sensory nodes and actuator nodes. Unless specified, variables are implied to belong to $\\n$ ($\\n.p=p$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RsVtTweIhnZ_"
   },
   "source": [
    "### Sensory Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D-TJLaMnrQPJ"
   },
   "source": [
    "Sensory and simply publish states:\n",
    "\n",
    "$\\overline{\\underline{\\textbf{Algorithm:}\\ \\text{Sensory Node }\\n{\\ Update}}}$\n",
    "\n",
    "$1.\\ \\s=\\text{get sensory data}$\n",
    "\n",
    "$6.\\ \\textbf{wait}\\ p\\ \\text{timesteps}$ \n",
    "\n",
    "$3.\\ \\textbf{repeat}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ijOK8JGFhcxy"
   },
   "source": [
    "### Actuator Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fwOrU7WGhH9w"
   },
   "source": [
    "Actuator nodes attempts to fulfill their childrens' target observations. At the actuator level, distributions must finally be sampled into real numbers.\n",
    "\n",
    "$\\overline{\\underline{\\textbf{Algorithm:}\\ \\text{Actuator Node }\\n{\\ Update}}}$\n",
    "\n",
    "$1.\\ \\starget[t+p]=\\text{weighted ave}(\\{\\langle w_{pred},\\spred[t+p]\\rangle\\} \\cup \\{\\langle \\n[ch].w_{\\mathcal{Pa}_i}\\CER(\\n[ch].\\otarget[\\tau])\\times e^{-\\n[ch].\\lambda(t+p-\\tau)}, \\n[ch].\\otarget[\\tau]\\rangle\\ |\\ \\exists\\n[ch].\\otarget[\\tau];\\ \\tau=\\max [t-N_o:t+p];\\ \\forall\\n[ch]\\in\\nchildren\\})$\n",
    "\n",
    "$2.\\ \\text{execute } \\overset{\\small{N_s}}{\\Ex}[\\starget[t+p]] \\quad\\rhd\\text{execute weighted target state sampling }N_s\\text{ times}$\n",
    "\n",
    "$3.\\ \\textbf{wait}\\ p\\ \\text{timesteps}$\n",
    "\n",
    "$4.\\ \\s[t+p]=\\text{get actual result}$\n",
    "\n",
    "$5.\\ \\textbf{repeat}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MCNWtrxMhgev"
   },
   "source": [
    "### Information Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iKPUWH4vrLev"
   },
   "source": [
    "Information nodes simulatenously engage in bottom-up processing and propagate top-down conditioning:\n",
    "\n",
    "$\\overline{\\underline{\\textbf{Algorithm:}\\ \\text{Information Node }\\n{\\ Update}}}$\n",
    "\n",
    "$1.\\ \\mathbf{store}\\ \\otrue=\\rm{concat}(\\n[parent].\\s|\\forall\\n[parent]\\in\\nparent)$\n",
    "\n",
    "$2.\\ d_t^{ave}=d_t^{ave}+\\alpha_{d,1}\\frac{I(\\otrue)}{H(\\s[t-p])}$\n",
    "\n",
    "$3.\\ \\falert=\\tanh[\\alpha_{d,2}(d_t^{ave}-\\alpha_{d,3})]$\n",
    "\n",
    "$4.\\ \\o=\\falert\\otrue+(1-\\falert)\\otarget[t]$\n",
    "\n",
    "$5.\\ \\s=\\ab(\\opast[t-N_o:t])$\n",
    "\n",
    "$6.\\ \\spred[t+p]=\\pr(\\s)$\n",
    "\n",
    "$7.\\ \\starget[t+p]=\\text{weighted ave}(\\{\\langle w_{pred},\\spred[t+p]\\rangle\\} \\cup \\{\\langle \\n[ch].w_{\\mathcal{Pa}_i}\\CER(\\n[ch].\\otarget[\\tau])\\times e^{-\\n[ch].\\lambda(t+p-\\tau)}, \\n[ch].\\otarget[\\tau]\\rangle\\ |\\ \\exists\\n[ch].\\otarget[\\tau];\\ \\tau=\\max [t-N_o:t+p];\\ \\forall\\n[ch]\\in\\nchildren\\})$\n",
    "\n",
    "$8.\\ \\otarget[t+p]=\\pol(\\s,\\starget[t+p])$\n",
    "\n",
    "$9.\\ \\textbf{wait}\\ p\\ \\text{timesteps}$ \n",
    "\n",
    "$10.\\ \\textbf{repeat}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zb-dUAFDiTCT"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nFsakK6pPkol"
   },
   "source": [
    "Training is performed online, on-policy, and on-predictor with new, prioritized, and imagined data. Minibatches are only one rollout sequence of observations ($N=1$) from every point in time in an episode to allow training large architectures and keep policies fresh. Dreaming provides novel observation trajectories for the abstractor and child information nodes to learn. However information nodes do not train their policy or predictor on its own dreamed trajectories ($\\o=\\otrue$ when optimizing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OvS8-M6QG-bD"
   },
   "source": [
    "### Learning the Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VfT1hBhEiUhv"
   },
   "source": [
    "$\\def\\loss{\\mathcal{L}}$\n",
    "$\\def\\obj{\\mathcal{J}}$\n",
    "The policy aims to produce target observations $\\otarget[t+p]=\\pol(\\s,\\starget[t+p])$ which cumulatively:\n",
    "- maximize target state achievement $D_{KL}\\bigl(\\starget[t+p]||\\ab([\\otruepast;\\pol(\\s,\\starget[t+p])])\\bigr)$\n",
    "- maximize resulting state certainty $\\CER\\bigl(\\ab([\\otruepast;\\pol(\\s,\\starget[t+p])])\\bigr)$\n",
    "- maximize informative observations targeted $\\CER\\bigl(\\pol(\\s,\\starget[t+p])\\bigr)$\n",
    "\n",
    "These objectives are combined as:\n",
    "\n",
    "$\\loss_\\pol=\\underbrace{\\beta_{\\pol,D}\\norm{\\ln{D_{KL}\\bigl(\\starget[t+p]||\\ab([\\otruepast;\\pol(\\s,\\starget[t+p])])\\bigr)}}}_\\text{target state achievement}-\\underbrace{\\beta_{\\pol,\\CER}\\norm{\\ln{\\CER\\bigl(\\ab([\\otruepast;\\pol(\\s,\\starget[t+p])])\\bigr)}}}_\\text{resulting state certainty}-\\underbrace{\\beta_{\\pol,I}\\norm{\\ln{\\CER\\bigl(\\pol(\\s,\\starget[t+p])\\bigr)}}}_\\text{informative observations targeted}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H28p8t8hp1Tm"
   },
   "source": [
    "This attempts to make the policy descend the gradient of predictive error and entropy while attempting to please its children's desired target observations. That is, it should produce a target observation which will make the abstractor produce an abstraction $\\ab([\\opast;\\pol(\\s,\\starget[t+p])])$ as close as possible to $\\starget[t+p]$. The last objective is added because informative events often reduce cumulative predictive entropy. By searching out unexpected, *informative* events, an agent is able to learn a more robust internal model $\\Pr(\\o[t+p]|\\opast)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gGKsPQCmHC9F"
   },
   "source": [
    "### Learning the Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZwPV0LwsJ-_J"
   },
   "source": [
    "Ideally, the predictor aims to model its observations perfectly $\\spred[t+p]=\\s[t+p]$. This means it should be able to maintain an accurate predicted state trajectory s.t. $\\pr(\\pr(\\pr(\\s)))\\approxeq \\s[t+3p]$. That requires maximizing predictive certainty $\\CER(\\pr(\\s))$. Jointly, these objectives compose:\n",
    "\n",
    "$\\obj_\\pr=\\underbrace{\\beta_{\\pr,D}\\norm{\\ln{\\bigl[D_{KL}(\\pr(\\s)||\\s[t+p])\\bigr]}}}_\\text{accuracy} - \\underbrace{\\beta_{\\pr,\\CER}\\norm{\\ln{ \\CER(\\pr(\\s))}}}_\\text{certainty}$\n",
    "\n",
    "Objectives are computed componentwise and only then normalized to encourage modeling only a few variables accurately rather than attempting to poorly model a joint distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oQ3MPUnUHK0Z"
   },
   "source": [
    "### Learning the Abstractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t7Wx0s_qHNAB"
   },
   "source": [
    "In some information nodes, the abstractor may be frozen with pretrained weights. When trainable however, its optimization attempts to capture variables relevant to both the policy's and predictor's losses. Although dreaming may provide novel observation trajectories for the abstractor to represent only true observations are used to optimize $\\ab$. However, child information nodes do optimize on the novel state trajectories generated by dreaming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6e5mzv0qPf6U"
   },
   "source": [
    "### Putting it all togethor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iADTqp0EQaoj"
   },
   "source": [
    "Optimization begins at each record in time of an information episoe $\\IE=\\langle\\otruepast,\\nchildren \\text{(target observation sequences only)}\\rangle$ by predicting the rollout for $T_{rollout}$ timesteps. Then cumulative policy and predictor losses are minimized by optimizing policy, predictor, and abstractor weights. Importantly, this optimization process takes place for nodes individually, allowing very large node architectures to integrate in an agent that no one processing unit could support."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cSEmbqUrp427"
   },
   "source": [
    "$\\begin{equation}\\overline{\\underline{\\mathbf{Algorithm}:\\ \\text{Train Information Node }\\n}} \\\\\n",
    "1.\\ \\ \\mathbf{for}\\ t\\in[\\IE.T_{start}+p,\\IE.T_{start}+2p,\\dots,\\IE.T_{end}]:\\qquad\\rhd\\text{precompute }\\s[ ],\\spred[ ],\\starget[ ],\\otarget[ ] \\\\\n",
    "2.\\ \\ \\quad \\s=\\ab(\\IE.\\otruepast) \\\\\n",
    "3.\\ \\ \\quad \\spred[t+p]=\\pr(\\s) \\\\\n",
    "4.\\ \\ \\quad \\starget[t+p]=\\text{weighted ave}(\\{\\langle w_{pred},\\spred[t+p]\\rangle\\} \\cup \\\\\n",
    "\\kern+7em\\{\\langle \\n[ch].w_{\\mathcal{Pa}_i}\\CER(\\n[ch].\\otarget[\\tau]) e^{-\\n[ch].\\lambda(t+p-\\tau)}, \\n[ch].\\otarget[\\tau]\\rangle\\ |\\ \\exists\\n[ch].\\otarget[\\tau];\\ \\tau=\\max [t-N_o:t+p];\\ \\forall\\n[ch]\\in\\IE.\\nchildren\\}) \\\\\n",
    "5.\\ \\ \\quad\\otarget[t+p]=\\pol(\\s,\\starget[t+p]) \\\\\n",
    "6.\\ \\ \\mathbf{end\\ for} \\\\\n",
    "7.\\ \\ \\mathbf{for}\\ t\\in[\\IE.T_{start}+p,\\IE.T_{start}+2p,\\dots,\\IE.T_{end}]: \\\\\n",
    "8.\\ \\ \\quad \\loss_t=\\underbrace{\\beta_{\\pol,D}\\norm{\\ln{D_{KL}\\bigl(\\starget[t+p]||\\ab([\\otruepast;\\pol(\\s,\\starget[t+p])])\\bigr)}}}_\\text{target state achievement}-\\underbrace{\\beta_{\\pol,\\CER}\\norm{\\ln{\\CER\\bigl(\\ab([\\otruepast;\\pol(\\s,\\starget[t+p])])\\bigr)}}}_\\text{resulting state certainty}-\\underbrace{\\beta_{\\pol,I}\\norm{\\ln{\\CER\\bigl(\\pol(\\s,\\starget[t+p])\\bigr)}}}_\\text{informative observations targeted} \\\\\n",
    "9.\\ \\ \\quad \\tau=t \\\\\n",
    "10.\\ \\quad \\oimagpast=\\otruepast \\\\\n",
    "11.\\ \\quad \\sblind[\\tau]=\\s \\\\\n",
    "12.\\ \\quad \\mathbf{while}\\ \\tau\\le t+T_{rollout}\\ \\mathbf{and}\\ \\tau\\le\\IE.T_{end}-p:\\qquad\\rhd\\text{compute rollout comparing against real data} \\\\\n",
    "13.\\ \\qquad \\simag=\\ab(\\oimagpast) \\\\\n",
    "14.\\ \\qquad \\spredimag[\\tau+p]=\\pr(\\simag) \\\\\n",
    "15.\\ \\qquad \\stargetimag[\\tau+p]=\\text{weighted ave}(\\{\\langle w_{predimag},\\spredimag[\\tau+p]\\rangle\\} \\cup \\{\\langle \\n[ch].w_{\\mathcal{Pa}_i}\\CER(\\n[ch].\\o[\\tau_{ch}])e^{-\\n[ch].\\lambda(\\tau+p-\\tau_{ch})}, \\n[ch].\\o[\\tau_{ch}]\\rangle \\\\\n",
    "\\kern+10em |\\ \\exists\\n[ch].\\o[\\tau_{ch}];\\ \\tau_{ch}=\\max [\\tau-N_o:\\tau+p];\\ \\forall\\n[ch]\\in\\IE.\\nchildren\\}) \\\\\n",
    "16.\\ \\qquad \\oimag[\\tau+p]=\\pol(\\simag,\\stargetimag) \\\\\n",
    "17.\\ \\qquad \\sblind=\\pr(\\sblind[\\tau]) \\\\\n",
    "18.\\ \\qquad \\loss_t=\\loss_t+\\gamma^{t-\\tau}\\Biggl[ \\underbrace{\\beta_{\\pol,D}\\norm{\\ln{D_{KL}(\\stargetimag[\\tau]||\\simag)}}}_\\text{target state achievement} - \\underbrace{\\beta_{\\pol,\\CER}\\norm{\\ln{\\CER(\\simag)}}}_\\text{resulting state certainty} - \\underbrace{\\beta_{\\pol,I}\\norm{\\ln{\\CER(\\oimag[\\tau+p])}}}_\\text{informative observations targeted} \\\\\n",
    "\\kern+6em + \\underbrace{\\beta_{\\pr,D}\\norm{\\ln{\\bigl[D_{KL}(\\sblind||\\s[t+p])}}}_\\text{blind prediction accuracy} - \\underbrace{\\beta_{\\pr,\\CER}\\norm{\\ln{ \\CER(\\sblind)}}}_\\text{blind prediction certainty}\\Biggr]\\\\\n",
    "19.\\ \\qquad \\tau = \\tau + p \\\\\n",
    "20.\\ \\ \\quad \\mathbf{end\\ while} \\\\\n",
    "21.\\ \\quad \\underset{\\theta_\\ab,\\theta_\\pr,\\theta_\\pol}\\min\\loss_{t}-R_t(s)\\qquad\\rhd\\text{minimize discounted sum loss and reward for timestep }t \\\\\n",
    "22.\\ \\ \\mathbf{end\\ for} \\end{equation}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l5qIvwPZUkIj"
   },
   "source": [
    "To minimize uncertainty, $\\beta_{\\pol,\\CER}>1$, $\\beta_{\\pr,\\CER}>1$. $R_t(s)$ encompasses any supervised rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aX9XVx_9J6dy"
   },
   "source": [
    "## Knowledge Transfer and Swarm Intelligence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-8eWKIEgKE2y"
   },
   "source": [
    "AGI-0's node-based architecture allows different networks to share underlying nodes and individual nodes to transfer in knowledge from other neural networks. For example, a software automation agent and a writing agent might benefit by sharing nodes that process natural language. Some natural language processing nodes might also utilize existing language models like gpt for their abstractor and a conditional language modeling head for the policy. Nodes do not belong to any one agent but instead a *swarm cloud* which trains and serves individual nodes. *Information episodes* $\\IE$ are collected from nodes after the agent's episode completes. This allows agents to synergize each other's heterogeneous architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0NUsETClOpkS"
   },
   "source": [
    "![swarm intelligence](https://mermaid.ink/img/eyJjb2RlIjoiZ3JhcGggVERcbiAgQVtwaXhlbHNdIC0tPiBCKHZpc3VhbCBkYXRhKVxuICBBIC0tPiBDXG4gIEIgLS0-IEMoc3ltYm9sIHN0cmluZ3MpXG4gIEMgLS0-IEQobGFuZ3VhZ2UgcHJvY2Vzc2luZylcblx0RCAtLT4gRShhYnN0cmFjdCB1bmRlcnN0YW5kaW5nKVxuICBGW2tleSBldmVudHNdIC0tPiBHXG4gIEhbbW91c2UgZXZlbnRzXSAtLT4gR1xuICBHKGFjdGlvbiBwcmltaXRpdmVzKVxuICBHIC0tPiBFXG4gIEYgLS0-IEkobGFuZ3VhZ2UgcHJvY2Vzc2luZylcbiAgSSAtLT4gRVxuXG4gIEpbdGV4dF0gLS0-IEtcbiAgSyhsYW5ndWFnZSBwcm9jZXNzaW5nKVxuICBLIC0tPiBMKGxhbmd1YWdlIHVuZGVyc3RhbmRpbmcgSSlcbiAgSyAtLT4gTShsYW5ndWFnZSB1bmRlcnN0YW5kaW5nIElJKVxuICBLIC0tPiBOKGxhbmd1YWdlIHVuZGVyc3RhbmRpbmcgSUlJKVxuICBMIC0tPiBOXG4gIE0gLS0-IE4iLCJtZXJtYWlkIjp7InRoZW1lIjoiZGVmYXVsdCJ9LCJ1cGRhdGVFZGl0b3IiOmZhbHNlfQ)\n",
    "<!-- https://mermaid-js.github.io/mermaid-live-editor/#/edit/eyJjb2RlIjoiZ3JhcGggVERcbiAgQVtwaXhlbHNdIC0tPiBCKHZpc3VhbCBkYXRhKVxuICBBIC0tPiBDXG4gIEIgLS0-IEMoc3ltYm9sIHN0cmluZ3MpXG4gIEMgLS0-IEQobGFuZ3VhZ2UgcHJvY2Vzc2luZylcblx0RCAtLT4gRShhYnN0cmFjdCB1bmRlcnN0YW5kaW5nKVxuICBGW2tleSBldmVudHNdIC0tPiBHXG4gIEhbbW91c2UgZXZlbnRzXSAtLT4gR1xuICBHKGFjdGlvbiBwcmltaXRpdmVzKVxuICBHIC0tPiBFXG4gIEYgLS0-IEkobGFuZ3VhZ2UgcHJvY2Vzc2luZylcbiAgSSAtLT4gRVxuXG4gIEpbdGV4dF0gLS0-IEtcbiAgSyhsYW5ndWFnZSBwcm9jZXNzaW5nKVxuICBLIC0tPiBMKGxhbmd1YWdlIHVuZGVyc3RhbmRpbmcgSSlcbiAgSyAtLT4gTShsYW5ndWFnZSB1bmRlcnN0YW5kaW5nIElJKVxuICBLIC0tPiBOKGxhbmd1YWdlIHVuZGVyc3RhbmRpbmcgSUlJKVxuICBMIC0tPiBOXG4gIE0gLS0-IE4iLCJtZXJtYWlkIjp7InRoZW1lIjoiZGVmYXVsdCJ9LCJ1cGRhdGVFZGl0b3IiOmZhbHNlfQ -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "awnn-OSaRJ0H"
   },
   "source": [
    "![similarities](https://mermaid.ink/img/eyJjb2RlIjoiZ3JhcGggVERcbiAgQVtvYnNlcnZhdGlvbnMgdXAgdG8gdF1cbiAgQltiaWFzaW5nIGluZm9ybWF0aW9uXVxuICBDW3RhcmdldCBvYnNlcnZhdGlvbl1cblxuICBEKGFic3RyYWN0b3IpXG4gIEUocHJlZGljdG9yKVxuICBGW3RhcmdldCBzdGF0ZV1cbiAgRyhwb2xpY3kpXG5cbiAgQSAtLT4gRFxuICBEIC0tPiBFXG4gIEUgLS0-IEZcbiAgQiAtLT4gRlxuXG4gIEYgLS0-IEdcbiAgRyAtLT4gQ1xuXG4gIElbdG9rZW4gc2VxdWVuY2VdIC0tPiBKKHRyYW5zZm9ybWVyKVxuICBKIC0tPiBLKGxhbmd1YWdlIG1vZGVsaW5nIGhlYWQpXG4gIEsgLS0-IExbbmV4dCB0b2tlbl0iLCJtZXJtYWlkIjp7InRoZW1lIjoiZGVmYXVsdCJ9LCJ1cGRhdGVFZGl0b3IiOmZhbHNlfQ)\n",
    "<!-- https://mermaid-js.github.io/mermaid-live-editor/#/edit/eyJjb2RlIjoiZ3JhcGggVERcbiAgQVtvYnNlcnZhdGlvbnMgdXAgdG8gdF1cbiAgQltiaWFzaW5nIGluZm9ybWF0aW9uXVxuICBDW3RhcmdldCBvYnNlcnZhdGlvbl1cblxuICBEKGFic3RyYWN0b3IpXG4gIEUocHJlZGljdG9yKVxuICBGW3RhcmdldCBzdGF0ZV1cbiAgRyhwb2xpY3kpXG5cbiAgQSAtLT4gRFxuICBEIC0tPiBFXG4gIEUgLS0-IEZcbiAgQiAtLT4gRlxuXG4gIEYgLS0-IEdcbiAgRyAtLT4gQ1xuXG4gIElbdG9rZW4gc2VxdWVuY2VdIC0tPiBKKHRyYW5zZm9ybWVyKVxuICBKIC0tPiBLKGxhbmd1YWdlIG1vZGVsaW5nIGhlYWQpXG4gIEsgLS0-IExbbmV4dCB0b2tlbl0iLCJtZXJtYWlkIjp7InRoZW1lIjoiZGVmYXVsdCJ9LCJ1cGRhdGVFZGl0b3IiOmZhbHNlfQ -->\n",
    "In the above comparison between a language modeling transformer and an information node, note that the predictor must be trained on supervised data. This could be implimented by initializing or even freezing the abstractor's weights with the transformer network, training the predictor to predict embedding transitions, and forcing the policy to produce the required next token from that predicted embedding (although top-down influence can change network output). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QZjiudV38IK3"
   },
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P44-_dzryyXK"
   },
   "source": [
    "## Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4ysgYLQ2Vb5w"
   },
   "source": [
    "Since nodes are asychronous, they can be trained or deployed on different underlying architectures without significant issues resulting from latency. Following is a generic node class and then sensory, actuator, and information node implimentations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FvV-iBd9dsO2"
   },
   "source": [
    "**ACTUALLY, EMBED TO FILE IN GITHUB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yl2u_f8ly2ln"
   },
   "outputs": [],
   "source": [
    "class Node:\n",
    "\n",
    "    self.children\n",
    "\n",
    "    self.p\n",
    "\n",
    "    self.o\n",
    "    self.s\n",
    "\n",
    "\n",
    "    def update():\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6cPLPCRmyvV3"
   },
   "source": [
    "### Sensory Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qvRXckvMy3uh"
   },
   "outputs": [],
   "source": [
    "class Sensory_Node(node):\n",
    "\n",
    "    def update():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "btVb_ICMy4PU"
   },
   "source": [
    "### Actuator Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E01YG6Axy6Ae"
   },
   "outputs": [],
   "source": [
    "class Actuator_Node(node):\n",
    "\n",
    "    def update():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i9sEaBWBy6zq"
   },
   "source": [
    "### Information Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t5N3Jlz6f-TU"
   },
   "outputs": [],
   "source": [
    "class Information_Episode:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mb1VdiQYy8vN"
   },
   "outputs": [],
   "source": [
    "class Information_Node(node):\n",
    "\n",
    "    self.parents\n",
    "\n",
    "    self.otarget\n",
    "\n",
    "    self.hparams\n",
    "\n",
    "    def update():\n",
    "        pass\n",
    "\n",
    "    def compile_information_episode() -> Information_Episode:\n",
    "        pass\n",
    "\n",
    "    def train(information_episode: Information_Episode):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e1Re05LMy_r-"
   },
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i3GF86wqzDOK"
   },
   "source": [
    "Brief overview of what's happening (maybe remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AE7qgBWHzItN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kFUUgMM-J4OR"
   },
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C17Mu5u0d4fR"
   },
   "source": [
    "Introduction\n",
    "\n",
    "Observing Nodal RL in\n",
    "- different environments\n",
    "- different agents\n",
    "- architecture deviations\n",
    "- unsupervised/guided exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dIDXARBpMUGd"
   },
   "source": [
    "The following experiment parameters are explored:\n",
    "\n",
    "### Generic Information Nodes:\n",
    "\n",
    "- Dense Information Node\n",
    "    - proof of concept node\n",
    "    - dense/specialized abstractor\n",
    "    - dense predictor\n",
    "    - dense policy\n",
    "\n",
    "- Sparse Information Node\n",
    "    - sparse variational autoencoders ensembles competing to represent $\\ab$ by lowest representation entropy \n",
    "    - sparse transformer $\\pr$\n",
    "    - optionally penalizing state certainty decrease and rewarding entropic actions\n",
    "    - directly minimizing entropy of binary valued sparse state vector quantization with annealing self organizing maps\n",
    "\n",
    "- Energy-based Information Node\n",
    "    - energy-based $\\ab$ and $\\pr$\n",
    "    - $pol$ analagous to 'energy descent with a bias'\n",
    "\n",
    "### Specialized Information Nodes\n",
    "\n",
    "- language modeling\n",
    "- machine vision\n",
    "- audio\n",
    "\n",
    "### Substructures\n",
    "\n",
    "- Homogeneous Ensembles\n",
    "- Heterogeneous Ensembles\n",
    "\n",
    "### Training\n",
    "\n",
    "- in-context supervised training and zero, one, and few shot test\n",
    "- penalizing activation\n",
    "- maximize $ln{[\\CER(\\s[t+p]) - \\CER(\\s)]}$\n",
    "\n",
    "### agents/environments\n",
    "\n",
    "- kinistetic\n",
    "    - one legged hopper\n",
    "    - spider\n",
    "    - humanoid\n",
    "\n",
    "- multimedia modeling\n",
    "    - image\n",
    "    - video\n",
    "    - audio\n",
    "\n",
    "- board games\n",
    "    - checkers\n",
    "    - chess\n",
    "    - go\n",
    "\n",
    "- software agent\n",
    "    - bash REPL\n",
    "    - python REPL\n",
    "        - autoML\n",
    "        - selfML\n",
    "\n",
    "- Multi-Organism Systems\n",
    "    - chatbots\n",
    "\n",
    "- MALMO\n",
    "    - single agent\n",
    "    - multi agent (self-play)\n",
    "    - multi-agent (human-play)\n",
    "\n",
    "- Second Life\n",
    "\n",
    "- engineering design\n",
    "\n",
    "- more to come\n",
    "\n",
    "### high-level conditioning\n",
    "- none (open ended learning)\n",
    "- RL agent\n",
    "- realtime human control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "We2mnwFVWGXY"
   },
   "source": [
    "Individual experiments are found in hyperlinked notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8cx9vZeE8MRW"
   },
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9HeBp_5mI127"
   },
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CPj3023AI51B"
   },
   "source": [
    "# Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iQKKhUD6JV5v"
   },
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rZKpsynJj8hx"
   },
   "source": [
    "General conclusion here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7YhDCa-Mj-7I"
   },
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dUT2V7RRkBQ3"
   },
   "source": [
    "- **How generalized can Nodel RL agents become?** Will agent performance continue improving as it acquires skills specialized for increasingly diverse domains? Can a humanoid robot controlled by Nodal RL perform reliably in the general human activity domain?\n",
    "- **Can an [imitator-imitator](https://arxiv.org/abs/1912.02875) framework be used to condition Nodel-RL agent behavior?**\n",
    "- **How could transformer neural networks learn when to dream ($\\NN_{transformer}(\\opast)$) instead of employing a blind rolling average ($\\text{Percieved Observation }\\o$)?**\n",
    "- **Can humans learn to dynamically condition agents to control their behavior?** Since high level conditioning data only provides a general handle on agent behavior, would a user commanding the agent in realtime be able to adjust his or her commands dynamically to more reliably achieve high level targets?\n",
    "- **Will centralized shared-parameter multiagent training efficently improve performance?** Although parameter sharing in reinforcement learning [generally accelerates learning in MARL](https://arxiv.org/abs/2005.13625), is it worth the cost to centralize training these huge architectures?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TSx3hiCdJXv9"
   },
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "atfuig3PlswB"
   },
   "source": [
    " - @book{Goodfellow-et-al-2016,\n",
    "    title={Deep Learning},\n",
    "    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},\n",
    "    publisher={MIT Press},\n",
    "    note={\\url{http://www.deeplearningbook.org}},\n",
    "    year={2016}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nCK3n1NWGZi6"
   },
   "source": [
    "# Appendix"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN4jXFbrx60jFnmyZybKS/y",
   "collapsed_sections": [
    "R3osJDsOWh9x",
    "EfzF3eBEHqsZ",
    "n-cwPGm9DZSO",
    "Q3eej_jcg5sg",
    "oQ3MPUnUHK0Z",
    "6e5mzv0qPf6U",
    "aX9XVx_9J6dy",
    "QZjiudV38IK3"
   ],
   "include_colab_link": true,
   "name": "NMRL.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
