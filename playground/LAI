# LAI

The Limboid artificial intelligence system (LAI) is highly autonomous robotic control system. It synergies information theory, deep reinforcement learning, and swarm intelligence in a distributed, cloud robotic control and training loop.

### Cognitive Processes

Bottom-up processing and top-down conditioning allow hierarchical **cognitive** processing and control.

for general use:

$\mathrm{rollout}(s_{t_0:t_f},o_{t_f+1:t_f+t_1}\ or\ \mathrm{DREAM},\Delta t_{rollout}, \theta_P, \theta_\pi)$$=\langle s_{t_f+1:t_f+\Delta t_{rollout}}, o_{t_f+1:t_f+\Delta t_{rollout}}\rangle$

$T_1$ does not have to be less than or equal to $t_f+\Delta t_{rollout}$

### Social Learning

Discusss swarm learning

- agents interacting online

### Supervised Module Training

To accelerate agent learning, modules are trained on domain specific episode datasets. Each domain specific episode is experienced as raw observations. However, optimization weights are varied for each module group. 

```mermaid
%% Example of sequence diagram
  graph LR
  	V1(pixels) --> V2(features) --> V3(objects)
  	
```

​	

```mermaid
classDiagram
      Animal <|-- Duck
      Animal <|-- Fish
      Animal <|-- Zebra
      Animal : +int age
      Animal : +String gender
      Animal: +isMammal()
      Animal: +mate()
      class Duck{
          +beakColor
          +swim()
          +quack()
      }
      class Fish{
          -sizeInFeet
          -canEat()
      }
      class Zebra{
          +is_wild
          +run()
      }
```

In addition to parallel swarm learning, individual agents also acquire behaviors and skills by **behavior modeling**. This technique relies on the cognitive system's minimization of predictive entropy. **Behavior module groups** attempt to predict what behaviors will be expressed. Paying attention to good behavior models trains behavior modules to produce goals that represent behaviors similar to the model's performance. Since there exist many desirable behaviors which may be mutually overlapping in context, individual behavior module spaces are initially created to predict relatively orthogonal values: obedience, care, and cooperation. From these permutations, respect for life, bodily respect, working with others and other behaviors can emerge. The adversarial nature of LAI's cognitive architecture means significantly fewer expert demonstrations are required for novel, yet appropriate behavior generation. Training data is drawn from both expert demonstrations and curated performances from within the population. During supervised behavior training, the agent's attention is forced on the behavior of a model and it tries to predict the model's behavior. I hope decomposing training by specific modules will help accelerate learning.

To actively maintain behavioral values, LAI agents are also trained to turn attention away from behavior of bad models so that these do not become predictable and then acted out. However, the longer and more frequently an agent observes bad behavior, the less qualified it becomes for directing autonomous behavior. The composite system may not exhibit bad behavior when it is only predicting its associates', but when that predictive model focuses on the agent's own behavior, it may drive the agent to follow (predictable) bad behavior. Degenerate **bad behavior modules** are continuously searched for in the behavior module pool and removed from production when identified as too accurate and precise in predicting bad behavior.

Rouge behavior may also emerge from the composite combination of modules and out of distribution environment. To identify this, complete agents (not just behavior modules) are sampled on an test set of extreme situations. These include:

- adaptation following simulated robot failure (leg joint dislocating, arm deforming under extreme load)
- simulated response to attack (by human, animal, or machine)
- simulated response to disaster (shooting, building fire)
- simulated obedience to conflicting direction (command to hurt humans)

### Self Evaluation

Finally, agents periodically engage in **self evaluation**. Modules belonging to the **self evaluation module subset $M_{self\ eval.}\sub M$** receive a reward when the agent responds with "yes" to set introspective questions $Q_{self\ eval.}$ such as:

- Am I doing my best?
- Is it worth being me?
- Are humans safe?
- Was that a good choice?

These are very loose questions with answers dramatically shaped by agent development. However, their grounding in natural language gives the human-oriented environment some influence over how these very human values are reasoned on. Their answer is determined by first seeding the agent's mind with the state at a quiet time $s_{t_{quiet}}$ with an audibly perceived question $q_i \in Q_{self\ eval.}$ and then allowing modules belonging to the self evaluation module subset to predict reasonable transitions from that state for a finite time $T_{self\ eval.}$. Finally, the resulting state of those modules is classified as representing an answer of yes or not yes.

$starting\ from\ s_{t_{end}}$ 

$initialize\ \mathcal{D}_{self\ eval.},\ \mathcal{L}_{self\ eval.}$

$\bold{for}\ \mathcal{E}_i\ \bold{in}\ \mathcal{E}:$

​		$T_{quiet} = \mathrm{quiet}(\mathcal{E})$

​		$\bold{for}\ t_{quiet}\ \bold{in}\ T_{quiet}:$

​				$\Delta t_{self\ eval.}=$$\Delta t\ \text{computer can manage}(M_{self\ eval.})$

​				$d_{self\ eval.}=s_{t_{quiet}-\Delta t_{self\ eval.}:t_{quiet}}$

​				$append\ d_{self\ eval.}\ to\ \mathcal{D}_{self\ eval.}$

$\bold{for}\ d_{self\ eval.}\ \bold{in}\ \mathcal{D}_{self\ eval.}:$

​		$sample\ q_i\ from\ Q_{self\ eval.}$ 

​		$sample\ \Delta t_{eval.}\ from\ \mathcal{N}(\mu_{\Delta t\ eval.},\sigma_{\Delta t\ eval.})$

​		$t_{end}=d_{self\ eval.}.t_{end}$

​		$o_{q_i}=\mathrm{simulate\ hearing}(q_i)$

​		$s_{self\ eval.}=$$\mathrm{rollout}(d_{self\ eval.},o_{q_i},o_{q_i}.\Delta t+\Delta t_{eval.}, \theta_{P,M_{self\ eval.}},\mathrm{STOPGRAD}(\theta_{\pi,M_{self\ eval.}})).s$

​		$append \mathcal{L}_{self\ eval.,i}=\ln(\mathrm{yes?}(s_{self\ eval.}))\ to\ \mathcal{L}_{self\ eval.}$

$\bold{return}\ \mathcal{L}_{self\ eval.}$

Since $\mathrm{STOPGRAD}(\theta_{M_{self\ eval.}})$ prevents future changes, only by changing past actions can the $\mathrm{yes?}$ score be improved. Some agents may feel trapped thinking there is little they can do to improve their situation: $\frac{d\mathcal{L}_{self\ eval.}}{d\theta_\pi}\approx0$. However, bottom up optimization gradually corrects deceptive thinking such as "this is all my fault" with "part of this is my fault". Gradients are frozen when processing the question $\mathrm{rollout}(\dots,\mathrm{STOPGRAD(\theta_M)})$ so that the modules' predictive models do not simply optimize their ability to lie 'yes' in any circumstance but report an honest answer. $\mathrm{yes?}$ is trained by supervised analysis of the agent's self evaluation modules' state traces following actual experiences where the agent says yes.

$initialize\ \mathcal{D}_{yes},\ \mathcal{D}_{no}$

$\bold{for}\ \mathcal{E_i}\ \bold{in}\ \mathcal{E}:$

​		$T_{yes}=\mathrm{occurances\ of\ yes}(\mathcal{E}_i)$

​		$\bold{for}\ t_{yes}\ \bold{in}\ T_{yes}:$

​				$sample\ \Delta t_{wait}\ from\ \mathcal{N}(\mu_{\Delta t\ answer},\sigma_{\Delta t\ answer})$

​				$add\ s_{t_{yes}:t_{yes}+\Delta t_{wait}}\ to\ \mathcal{D}_{yes}$

​		$T_{no}=\mathrm{occurances\ of\ no}(\mathcal{E}_i)$

​		$\bold{for}\ t_{no}\ \bold{in}\ T_{no}:$

​				$sample\ \Delta t_{wait}\ from\ \mathcal{N}(\mu_{\Delta t\ answer},\sigma_{\Delta t\ answer})$

​				$add\ s_{t_{no}:t_{no}+\Delta t_{wait}}\ to\ \mathcal{D}_{no}$

$\underset{\theta_\mathrm{yes?}}{\mathrm{fit}}\ \mathcal{L}(\mathrm{yes?},[\mathcal{D}_{yes};\mathcal{D}_{no}],[\bold{1}^{\|\mathcal{D}_{yes}\|};\bold{0}^{\|\mathcal{D}_{yes}\|}])$