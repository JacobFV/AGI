{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tfk = tf.keras\n",
    "tfkl = tfk.layers\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "tfpl = tfp.layers\n",
    "tfb = tfp.bijectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphNet(tfk.Model):\n",
    "    \n",
    "    def __init__(self,\n",
    "        f_inp,\n",
    "        f_pool,\n",
    "        f_v_up,\n",
    "        f_e_up,\n",
    "        f_adj_up,\n",
    "        **kwargs):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        \n",
    "        super(GraphNet, self).__init__(kwargs)\n",
    "        \n",
    "        self.f_inp = f_inp\n",
    "        self.f_pool = f_pool\n",
    "        self.f_v_up = f_v_up\n",
    "        self.f_e_up = f_e_up\n",
    "        self.f_adj_up = f_adj_up\n",
    "        \n",
    "        self.f_V_src_loc = tfkl.Lambda(lambda V_src, A: \n",
    "            tf.einsum('...sv,...sd->...sdv', V_src, A))\n",
    "        self.f_V_dst_loc = tfkl.Lambda(lambda V_dst, A: \n",
    "            tf.einsum('...dv,...sd->...dsv', V_src_loc))\n",
    "        self.f_perm = tfkl.Lambda(lambda x: tf.einsum('...sdv->...dsv', x))\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        \n",
    "        # unpack inputs\n",
    "        V_src, V_dst, E, A = inputs\n",
    "        \n",
    "        # provide vert-localized copies of src and dst verts\n",
    "        V_src_loc = self._f_V_src_loc(V_src, A)\n",
    "        V_dst_loc = self._f_V_dst_loc(V_dst, A)\n",
    "        \n",
    "        # get src-dst pair-specific inputs to dst verts\n",
    "        inp = self.f_inp([V_src_loc, E], training=training)\n",
    "        inp = self.f_perm(inp)\n",
    "        \n",
    "        # pool src-dst pair-specific inputs\n",
    "        V_dst_new = self.f_pool([V_dst, inp], training=training)    \n",
    "        \n",
    "        # update dst verts\n",
    "        V_dst = self.f_v_up([V_dst, V_dst_new], training=training)\n",
    "        \n",
    "        # update edges\n",
    "        E = self.f_e_up([V_src_loc, V_dst_loc, E], training=training)\n",
    "        \n",
    "        # update adjacency matrix\n",
    "        A = self.f_a_up(E, training=training)\n",
    "        \n",
    "        return V_dst, E, A\n",
    "    \n",
    "        \n",
    "    @staticmethod\n",
    "    def f_pool_sum():\n",
    "        return tfkl.Lambda(lambda V_dst, inp: tf.reduce_sum(inp, axis=-2))\n",
    "    @staticmethod\n",
    "    def f_pool_ave():\n",
    "        return tfkl.Lambda(lambda V_dst, inp: tf.reduce_mean(inp, axis=-2))\n",
    "    @staticmethod\n",
    "    def f_pool_prod():\n",
    "        return tfkl.Lambda(lambda V_dst, inp: tf.reduce_prod(inp, axis=-2))\n",
    "    class f_pool_attn(tfkl.Layer):\n",
    "        \n",
    "        def __init__(self, d_key=8, d_val=None, N_heads=8, pre_layer_normalization=True, **kwargs):\n",
    "            \"\"\"\n",
    "            pre-LN (https://arxiv.org/abs/2004.08249)\n",
    "            \"\"\"\n",
    "            super(f_pool_attn, self).__init__(**kwargs)\n",
    "            \n",
    "            self.pre_layer_normalization = pre_layer_normalization                \n",
    "            self.d_key = d_key\n",
    "            self.d_val = self.d_key if d_val is None else d_val\n",
    "            self.N_heads = N_heads\n",
    "            \n",
    "            if self.pre_layer_normalization:\n",
    "                self.V_dst_LN = tfkl.LayerNormalization()\n",
    "                self.inp_LN = tfkl.LayerNormalization()\n",
    "            \n",
    "        def build(self, input_shape):\n",
    "            V_dst_shape, inp_shape = input_shape\n",
    "            \n",
    "            self.f_val = tfkl.Dense(N_heads * self.d_val, 'relu')\n",
    "            self.f_key = tfkl.Dense(N_heads * self.d_key, 'relu')\n",
    "            self.f_query = tfkl.Dense(N_heads * self.d_key, 'relu')\n",
    "            \n",
    "            self.reshape_q = tfkl.Reshape(V_dst_shape[:-2] +\n",
    "                (self.N_heads, self.d_key))\n",
    "            self.reshape_k = tfkl.Reshape(inp_shape[:-2] +\n",
    "                (self.N_heads, self.d_key))\n",
    "            self.reshape_v = tfkl.Reshape(inp_shape[:-2] +\n",
    "                (self.N_heads, self.d_val))\n",
    "            \n",
    "            def _f_MHA(queries, keys, values):\n",
    "                score = tf.einsum('...dhq,...dshq->dsh', queries, keys)\n",
    "                score = score / tf.sqrt(self.d_key)\n",
    "                score = tf.nn.softmax(score, axis=-1)\n",
    "                return tf.einsum('...dsh,...dshv->...dhv', score, values)\n",
    "            self.f_MHA = tfkl.Lambda(lambda q,k,v: _f_MHA(q,k,v))\n",
    "            \n",
    "            self.f_cat = tfkl.Reshape(V_dst_shape[:-1]+(-1,))\n",
    "            self.f_emb_cat = tfkl.Dense(V_dst_shape[-1], 'relu')\n",
    "        \n",
    "        def call(self, inputs, training=False):\n",
    "            # unpack inputs\n",
    "            V_dst, inp = inputs\n",
    "\n",
    "            # pre-LN\n",
    "            if pre_layer_normalization:\n",
    "                V_dst = self.V_dst_LN(V_dst, training=training)\n",
    "                inp = self.inp_LN(inp, training=training)\n",
    "            \n",
    "            # generate queries, keys, and values for all heads\n",
    "            queries = self.f_query(V_dst, training=training)  # [..., N_dst, N_heads*d_key]\n",
    "            keys = self.f_key(inp, training=training) # [..., N_dst, N_src, N_heads*d_key]\n",
    "            values = self.f_val(inp, training=training) # [..., N_dst, N_src, N_heads*d_val]\n",
    "            \n",
    "            # reshape into separate heads\n",
    "            queries = self.reshape_q(queries) # [..., N_dst, N_heads, d_key]\n",
    "            keys = self.reshape_k(keys) # [..., N_dst, N_heads, d_key]\n",
    "            values = self.reshape_v(values) # [..., N_dst, N_heads, d_key]\n",
    "            \n",
    "            # perform multi-head attention\n",
    "            mha_lookup = self.f_MHA([queries, keys, values], training=training)\n",
    "            # [..., N_dst, N_heads, d_val]\n",
    "            \n",
    "            # concatenate heads\n",
    "            mha_cat = self.f_cat(mha_lookup, training=training)\n",
    "            \n",
    "            # embed in output space\n",
    "            return self.f_emb_cat(mha_cat, training=training)\n",
    "        \n",
    "    @staticmethod\n",
    "    def f_v_up_add():\n",
    "        return tfkl.Add()\n",
    "    \n",
    "    @staticmethod\n",
    "    def f_v_up_direct():\n",
    "        return tfkl.Lambda(lambda V_dst, V_dst_new: V_dst_new)\n",
    "    \n",
    "    class f_v_up_beta(tfkl.Layer):\n",
    "        def __init__(self, **kwargs):\n",
    "            super(f_v_up_beta, self).__init__(**kwargs)\n",
    "            self.f_beta = tfkl.Dense(1, 'softmax')\n",
    "        def call(self, inputs, training=False):\n",
    "            V_dst, V_dst_new = inputs\n",
    "            beta = self.f_beta(V_dst_new)\n",
    "            return beta*V_dst + (1-beta)*V_dst_new\n",
    "        \n",
    "    class f_v_up_alphabeta(tfkl.Layer):\n",
    "        def __init__(self, **kwargs):\n",
    "            super(f_v_up_beta, self).__init__(**kwargs)\n",
    "            self.f_beta = tfkl.Dense(1, 'softmax')\n",
    "            self.f_alpha = tfkl.Dense(1, 'softmax')\n",
    "        def call(self, inputs, training=False):\n",
    "            V_dst, V_dst_new = inputs\n",
    "            alpha = self.f_alpha(V_dst)\n",
    "            beta = self.f_beta(V_dst_new)\n",
    "            return alpha*V_dst + beta*V_dst_new\n",
    "        \n",
    "    @staticmethod\n",
    "    def f_inp_concat():\n",
    "        return tfkl.Concatenate()\n",
    "    \n",
    "    @staticmethod\n",
    "    def f_inp_edges():\n",
    "        return tfkl.Lambda(lambda V_src_loc, E: E)\n",
    "    \n",
    "    @staticmethod\n",
    "    def f_inp_verts():\n",
    "        return tfkl.Lambda(lambda V_src_loc, E: V_src_loc)\n",
    "    \n",
    "    @staticmethod\n",
    "    def f_a_up():\n",
    "        def f(x):\n",
    "            y=tfkl.Dense(1, 'softmax')(x)\n",
    "            y=tf.squeeze(y)\n",
    "            \n",
    "        return tfkl.Lambda(lambda E: f(E))\n",
    "    \n",
    "    @staticmethod\n",
    "    def f_e_up_const():\n",
    "        return tfkl.Lambda(lambda V_src_loc, V_dst_loc, E: E)\n",
    "    \n",
    "    class f_e_up_dense(tfkl.Layer):\n",
    "        def __init__(self, **kwargs):\n",
    "            super(f_v_up_beta, self).__init__(**kwargs)\n",
    "        def build(self, input_shape):\n",
    "            V_src_loc_shape, V_dst_loc_shape, E_shape = input_shape\n",
    "            self.f_E_new = tfkl.Dense(tf.shape(E_shape)[-1], 'relu')\n",
    "            self.V_dst_perm = tfkl.Lambda(\n",
    "                lambda x: tf.einsum('...dsv->...sdv', x))\n",
    "        def call(self, inputs, training=False):\n",
    "            V_src_loc, V_dst_loc, E = inputs\n",
    "            V_dst_loc_perm = self.V_dst_perm(V_dst_loc)\n",
    "            return self.f_E_new(tfkl.concatenate([\n",
    "                V_src_loc, V_dst_loc_perm, E]))\n",
    "        \n",
    "    class f_e_up_dense_oneway(tfkl.Layer):\n",
    "        def __init__(self, **kwargs):\n",
    "            super(f_v_up_beta, self).__init__(**kwargs)\n",
    "        def build(self, input_shape):\n",
    "            V_src_loc_shape, V_dst_loc_shape, E_shape = input_shape\n",
    "            self.V_dst_perm = tfkl.Lambda(\n",
    "                lambda x: tf.einsum('...dsv->...sdv', x))\n",
    "            self.f_E_new = tfkl.Dense(tf.shape(E_shape)[-1], 'relu')\n",
    "        def call(self, inputs, training=False):\n",
    "            V_src_loc, V_dst_loc, E = inputs\n",
    "            return self.f_E_new(tfkl.concatenate([V_src_loc, E]))\n",
    "        \n",
    "    class f_e_up_beta(tfkl.Layer):\n",
    "        def __init__(self, **kwargs):\n",
    "            super(f_v_up_beta, self).__init__(**kwargs)\n",
    "        def build(self, input_shape):\n",
    "            V_src_loc_shape, V_dst_loc_shape, E_shape = input_shape\n",
    "            self.V_dst_perm = tfkl.Lambda(\n",
    "                lambda x: tf.einsum('...dsv->...sdv', x))\n",
    "            self.f_beta = tfkl.Dense(1, 'softmax')\n",
    "            self.f_E_new = tfkl.Dense(tf.shape(E_shape)[-1], 'relu')\n",
    "        def call(self, inputs, training=False):\n",
    "            V_src_loc, V_dst_loc, E = inputs\n",
    "            V_dst_loc_perm = self.V_dst_perm(V_dst_loc)\n",
    "            E_new = self.f_E_new(tfkl.concatenate([\n",
    "                V_src_loc, V_dst_loc_perm, E]))\n",
    "            beta = self.f_beta(tfkl.concatenate([V_src_loc, V_dst_loc_perm]))\n",
    "            return beta*V_dst + (1-beta)*E_new\n",
    "\n",
    "    class f_e_up_attn(tfkl.Layer):\n",
    "        \n",
    "        def __init__(self, d_key=8, d_val=None, N_heads=8, pre_layer_normalization=True, **kwargs):\n",
    "            \"\"\"\n",
    "            pre-LN (https://arxiv.org/abs/2004.08249)\n",
    "            \"\"\"\n",
    "            super(f_pool_attn, self).__init__(**kwargs)\n",
    "            \n",
    "            self.pre_layer_normalization = pre_layer_normalization                \n",
    "            self.d_key = d_key\n",
    "            self.d_val = self.d_key if d_val is None else d_val\n",
    "            self.N_heads = N_heads\n",
    "            \n",
    "            if self.pre_layer_normalization:\n",
    "                self.V_dst_LN = tfkl.LayerNormalization()\n",
    "                self.inp_LN = tfkl.LayerNormalization()\n",
    "            \n",
    "        def build(self, input_shape):\n",
    "            V_src_loc_shape, V_dst_loc_shape, E_shape = input_shape\n",
    "            \n",
    "            self.V_dst_perm = tfkl.Lambda(\n",
    "                lambda x: tf.einsum('...dsv->...sdv', x))\n",
    "            \n",
    "            self.cat_q_data = tfkl.Concatenate()\n",
    "            self.cat_kv_data = tkfl.Concatenate()\n",
    "            \n",
    "            self.f_val = tfkl.Dense(N_heads * self.d_val, 'relu')\n",
    "            self.f_key = tfkl.Dense(N_heads * self.d_key, 'relu')\n",
    "            self.f_query = tfkl.Dense(N_heads * self.d_key, 'relu')\n",
    "            \n",
    "            self.reshape_q = tfkl.Reshape(E_shape[:-1] +\n",
    "                (self.N_heads, self.d_key))\n",
    "            self.reshape_k = tfkl.Reshape(E_shape[:-1] +\n",
    "                (self.N_heads, self.d_key))\n",
    "            self.reshape_v = tfkl.Reshape(E_shape[:-1] +\n",
    "                (self.N_heads, self.d_val))\n",
    "            \n",
    "            def _f_MHA(queries, keys, values):\n",
    "                score = tf.einsum('...sdhq,...sdhq->sdh', queries, keys)\n",
    "                score = score / tf.sqrt(self.d_key)\n",
    "                score = tf.nn.softmax(score, axis=-1)\n",
    "                return tf.einsum('...sdh,...sdhv->...dhv', score, values)\n",
    "            self.f_MHA = tfkl.Lambda(lambda q,k,v: _f_MHA(q,k,v))\n",
    "            \n",
    "            self.f_cat = tfkl.Reshape(E_shape[:-1]+(-1,))\n",
    "            self.f_emb_cat = tfkl.Dense(E_shape[-1], 'relu')\n",
    "        \n",
    "        def call(self, inputs, training=False):\n",
    "            # unpack inputs\n",
    "            V_src_loc, V_dst_loc, E = inputs\n",
    "\n",
    "            # pre-LN\n",
    "            if pre_layer_normalization:\n",
    "                V_dst = self.V_dst_LN(V_dst, training=training)\n",
    "                inp = self.inp_LN(inp, training=training)\n",
    "            \n",
    "            V_dst_loc_perm = self.V_dst_perm(V_dst_loc)\n",
    "            \n",
    "            q_data = self.cat_q_data([V_dst_loc, E])\n",
    "            kv_data = self.cat_kv_data([V_src_loc, E])\n",
    "            \n",
    "            # generate queries, keys, and values for all heads\n",
    "            queries = self.f_query(q_data, training=training)  # [..., N_src, N_dst, N_heads*d_key]\n",
    "            keys = self.f_key(kv_data, training=training) # [..., N_src, N_dst, N_heads*d_key]\n",
    "            values = self.f_val(kv_data, training=training) # [..., N_src, N_dst, N_heads*d_val]\n",
    "            \n",
    "            # reshape into separate heads\n",
    "            queries = self.reshape_q(queries) # [..., N_src, N_dst, N_heads, d_key]\n",
    "            keys = self.reshape_k(keys) # [..., N_src, N_dst, N_heads, d_key]\n",
    "            values = self.reshape_v(values) # [..., N_src, N_dst, N_heads, d_key]\n",
    "            \n",
    "            # perform multi-head attention\n",
    "            mha_lookup = self.f_MHA([queries, keys, values], training=training)\n",
    "            # [..., N_src, N_dst, N_heads, d_val]\n",
    "            \n",
    "            # concatenate heads\n",
    "            mha_cat = self.f_cat(mha_lookup, training=training)\n",
    "            # [..., N_src, N_dst, N_heads*d_val]\n",
    "            \n",
    "            # embed in output space\n",
    "            return self.f_emb_cat(mha_cat, training=training)\n",
    "            # [..., N_src, N_dst, d_E]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class MultiGraphNet(tfk.Model):\n",
    "    \n",
    "    def __init__(self,\n",
    "        multigraph,\n",
    "        f_update_seq,\n",
    "        f_rel_update=None,\n",
    "        f_rel_update_model=None,\n",
    "        randomized_update_seq=False,\n",
    "        f_ret=(lambda x: x)):\n",
    "        \"\"\"\n",
    "        f_rel_update (dict<(str,str): GraphNet): update functions\n",
    "            for each source-destination graph pairs. If `None`, specify\n",
    "            an `f_rel_update_model` that will be applied to all edges in\n",
    "            the multigraph.\n",
    "        f_rel_update_model (GraphNet): updating function to be copied\n",
    "            for all source-destination graph relations in the case that\n",
    "            `f_rel_update` is `None`.\n",
    "        \"\"\"\n",
    "        self.MG = multigraph\n",
    "        self.update_seq = update_seq\n",
    "        self.f_rel_update = self.f_rel_update\n",
    "        if self.f_rel_update is None:\n",
    "            self.f_rel_update = {}\n",
    "            for (src, dst) in list(self.MG.Es.keys()):\n",
    "                self.f_rel_update[(src, dst)] = f_rel_update_model\n",
    "        self.randomized_update_seq = randomized_update_seq\n",
    "        self.f_ret = f_ret\n",
    "    \n",
    "    @staticmethod\n",
    "    def f_update_seq_reg(multigraph):\n",
    "        \"\"\"just go through all defined relations\"\"\"\n",
    "        seq = list(self.mg.Vs.keys())\n",
    "        if self.randomized_update_seq:\n",
    "            seq = random.shuffle(seq)\n",
    "        return seq\n",
    "    \n",
    "    @staticmethod\n",
    "    def f_update_seq_egocentric(multigraph):\n",
    "        \"\"\"first perform intragraph update, then intergraph update\"\"\"\n",
    "        \n",
    "        all_names = all_names2 = list(self.mg.Vs.keys())\n",
    "        if self.randomized_update_seq:\n",
    "            all_names = random.shuffle(all_names)\n",
    "        if self.randomized_update_seq:\n",
    "            all_names2 = random.shuffle(all_names2)\n",
    "            \n",
    "        intragraph_pairs = [(src_name, src_name) for src_name in all_names]\n",
    "        \n",
    "        intergraph_pairs = []\n",
    "        for src_name in all_names:\n",
    "            for dst_name in all_names:\n",
    "                if src_name != dst_name:\n",
    "                    intergraph_pairs.append((src_name, dst_name))\n",
    "                    \n",
    "        return intragraph_pairs + intergraph_pairs\n",
    "    \n",
    "    @staticmethod\n",
    "    class f_ret_just_graph:\n",
    "        def __init__(self, graph_name):\n",
    "            self.graph_name = graph_name\n",
    "        \n",
    "        def __call__(self, multigraph):\n",
    "            return (multigraph.Vs[self.graph_name],\n",
    "                    multigraph.Es[(self.graph_name, self.graph_name)],\n",
    "                    multigraph.As[(self.graph_name, self.graph_name)])\n",
    "        \n",
    "    @staticmethod\n",
    "    class f_ret_just_root:\n",
    "        def __init__(self, root_name):\n",
    "            self.root_name = root_name\n",
    "        \n",
    "        def __call__(self, multigraph):\n",
    "            return tf.reduce_mean(\n",
    "                multigraph.Vs[self.root_name],\n",
    "                axis=-2)\n",
    "    \n",
    "    def cell(self, inputs, training):\n",
    "        for rel in self.update_seq(self.mg):\n",
    "            src, dst = rel\n",
    "            self.MG.V[dst], self.MG.E[rel], self.MG.A[rel] = \\\n",
    "                self.f_rel_update[rel](\n",
    "                    self.MG.V[src], self.MG.V[dst],\n",
    "                    self.MG.E[rel], self.MG.A[rel])\n",
    "        return self.f_ret(self.mg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiGraph:\n",
    "    \n",
    "    def __init__(self, Vs=dict(), Es=dict(), As=dict()):\n",
    "        \"\"\"\n",
    "        Vs: dict<str, Tensor>\n",
    "        rels: dict<(str,str), Tensor>\n",
    "        \n",
    "        \"\"\"\n",
    "        self.Vs = Vs\n",
    "        self.Es = Es\n",
    "        self.As = As\n",
    "    \n",
    "    @property\n",
    "    def N_v(self, name):\n",
    "        return tf.shape(self.Vs[name])[-2]\n",
    "    @property\n",
    "    def d_v(self, name):\n",
    "        return tf.shape(self.Vs[name])[-1]\n",
    "    @property\n",
    "    def d_e(self, src, dst):\n",
    "        return tf.shape(self.Es[(src, dst)])[-1]\n",
    "    \n",
    "    def connect_graphs(self, src, dst, e_emb=[1.0], density=1.0):\n",
    "        leading_dims = tf.shape(self.Vs[src])[:-2]\n",
    "        N_src = tf.shape(self.Vs[src])[-2:-1]\n",
    "        N_dst = tf.shape(self.Vs[dst])[-2:-1]\n",
    "        self.As[src,dst] = tf.cast(tf.random.uniform(\n",
    "            shape=leading_dims+N_src+N_dst,\n",
    "            lowvalue=0, highvalue=1) < density,\n",
    "            tfk.backend.floatx)\n",
    "        self.Es[src,dst] = tf.einsum('...sd,v->...sdv',\n",
    "            self.As[src,dst], e_emb)\n",
    "    \n",
    "    def add_root_network(self,\n",
    "        root_name,\n",
    "        intragraph_density=1.0,\n",
    "        intergraph_density=1.0,\n",
    "        neighbors=[],\n",
    "        connection_direction=[\"src\", \"dst\"],\n",
    "        N_v=1,\n",
    "        emb_v=[1.0]):\n",
    "        \"\"\"convenience function to make root node network\n",
    "        and connect to other graphs. Root networks provide an\n",
    "        information highway for intragraph vert updates and\n",
    "        can be used to connect heterogenous graphs.\n",
    "        \n",
    "        WARNING: the multigraph must have at least one other\n",
    "        set of verts so we can detirmine batch size and time\n",
    "        steps (or any other leading dimensions).\n",
    "        \n",
    "        neighbors (list<str>): neighboring graphs (if any) to\n",
    "            connect new root network to\n",
    "        \"\"\"\n",
    "        \n",
    "        # create root graph verts\n",
    "        leading_dims = tf.shape(list(self.Vs.values())[0])[:-2]\n",
    "        d_v = tf.shape(emb_v)\n",
    "        self.Vs[root_name] = tf.fill(\n",
    "            dims=leading_dims+tf.TensorSpec((N_v,))+d_v,\n",
    "            value=emb_v)\n",
    "        \n",
    "        # connect graph internally\n",
    "        self.connect_graphs(root_name, root_name, \n",
    "            density=intragraph_density)\n",
    "        \n",
    "        # connect with neighbors\n",
    "        for neighbor in neighbors:\n",
    "            if \"src\" in connection_direction:\n",
    "                self.connect_graphs(root_name, neighbor,\n",
    "                    density=intergraph_density)\n",
    "            if \"dst\" in connection_direction:\n",
    "                self.connect_graphs(neighbor, root_name,\n",
    "                    density=intergraph_density)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
