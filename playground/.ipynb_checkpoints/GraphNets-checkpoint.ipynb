{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tfk = tf.keras\n",
    "tfkl = tfk.layers\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "tfpl = tfp.layers\n",
    "tfb = tfp.bijectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LogicalDevice(name='/device:CPU:0', device_type='CPU'),\n",
       " LogicalDevice(name='/device:XLA_CPU:0', device_type='XLA_CPU'),\n",
       " LogicalDevice(name='/device:GPU:0', device_type='GPU'),\n",
       " LogicalDevice(name='/device:XLA_GPU:0', device_type='XLA_GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_logical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphNet(tfk.Model):\n",
    "    \n",
    "    def __init__(self,\n",
    "        f_inp,\n",
    "        f_pool,\n",
    "        f_v_up,\n",
    "        f_e_up,\n",
    "        f_adj_up,\n",
    "        **kwargs):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        \n",
    "        super(GraphNet, self).__init__(kwargs)\n",
    "        \n",
    "        self.f_inp = f_inp\n",
    "        self.f_pool = f_pool\n",
    "        self.f_v_up = f_v_up\n",
    "        self.f_e_up = f_e_up\n",
    "        self.f_adj_up = f_adj_up\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        \n",
    "        # unpack inputs\n",
    "        V_src, V_dst, E, A = inputs\n",
    "        \n",
    "        # provide vert-localized copies of src and dst verts\n",
    "        V_src_loc = tf.einsum('...sv,...sd->...sdv', V_src, A)\n",
    "        V_dst_loc = tf.einsum('...sdv->...dsv', V_src_loc)\n",
    "        \n",
    "        # get src-dst pair-specific inputs to dst verts\n",
    "        inp = self.f_inp([V_src_loc, E], training=training)\n",
    "        inp = tf.einsum('...sdv->...dsv', inp)\n",
    "        \n",
    "        # pool src-dst pair-specific inputs\n",
    "        V_dst_new = self.f_pool([V_dst, inp], training=training)    \n",
    "        \n",
    "        # update dst verts\n",
    "        V_dst = self.f_v_up([V_dst, V_dst_new], training=training)\n",
    "        \n",
    "        # update edges\n",
    "        E = self.f_e_up([V_src, V_dst, E], training=training)\n",
    "        \n",
    "        # update adjacency matrix\n",
    "        A = self.f_a_up(E, training=training)\n",
    "        \n",
    "        return V_dst, E, A\n",
    "    \n",
    "        \n",
    "    @staticmethod\n",
    "    def f_pool_sum():\n",
    "        return tfkl.Lambda(lambda V_dst, inp: tf.reduce_sum(inp, axis=-2))\n",
    "    @staticmethod\n",
    "    def f_pool_ave():\n",
    "        return tfkl.Lambda(lambda V_dst, inp: tf.reduce_mean(inp, axis=-2))\n",
    "    @staticmethod\n",
    "    def f_pool_prod():\n",
    "        return tfkl.Lambda(lambda V_dst, inp: tf.reduce_prod(inp, axis=-2))\n",
    "    class f_pool_attn(tfkl.Layer):\n",
    "        \n",
    "        def __init__(self, d_val, d_key=8, pre_layer_normalization=True, **kwargs):\n",
    "            \"\"\"\n",
    "            pre-LN (https://arxiv.org/abs/2004.08249)\n",
    "            \"\"\"\n",
    "            super(f_pool_attn, self).__init__(**kwargs)\n",
    "            \n",
    "            self.pre_layer_normalization = pre_layer_normalization\n",
    "            if self.pre_layer_normalization:\n",
    "                self.V_dst_LN = tfkl.LayerNormalization()\n",
    "                self.inp_LN = tfkl.LayerNormalization()\n",
    "                \n",
    "            self.d_val = d_val\n",
    "            self.d_key = d_key\n",
    "            self.f_val = tfkl.Dense(d_val, 'relu')\n",
    "            self.f_key = tfkl.Dense(d_key, 'relu')\n",
    "            self.f_query = tfkl.Dense(d_key, 'relu')\n",
    "        \n",
    "        def call(self, inputs, training=False)\n",
    "            # unpacking\n",
    "            V_dst, inp = inputs\n",
    "\n",
    "            # pre-LN\n",
    "            if pre_layer_normalization:\n",
    "                V_dst = self.V_dst_LN(V_dst)\n",
    "                inp = self.inp_LN(inp)\n",
    "            \n",
    "            # generate queries, keys, and values\n",
    "            queries = self.f_query(V_dst)  # [..., N_dst, d_key]\n",
    "            keys = self.f_key(inp) # [..., N_dst, N_src, d_key]\n",
    "            vals = self.f_val(inp) # [..., N_dst, N_src, d_val]\n",
    "            \n",
    "            # attention\n",
    "            score = tf.einsum('...dq,...dsq->ds', queries, keys)\n",
    "            score = score / tf.sqrt(self.d_key)\n",
    "            score = tf.nn.softmax(score, axis=-1)\n",
    "            \n",
    "            # weighted sum\n",
    "            pooled = tf.einsum('...ds,...dsv->...dv', score, vals)\n",
    "            \n",
    "            return pooled               \n",
    "            \n",
    "    @staticmethod\n",
    "    def f_v_up_sum():\n",
    "        return tfkl.Add()\n",
    "    \n",
    "    @staticmethod\n",
    "    def f_v_up_direct():\n",
    "        return tfkl.Lambda(lambda V_dst, V_dst_new: return V_dst_new)\n",
    "    \n",
    "    class f_v_up_beta(tfkl.Layer):\n",
    "        def __init__(self, **kwargs):\n",
    "            super(f_v_up_beta, self).__init__(**kwargs)\n",
    "            self.f_beta = tfkl.Dense(1, 'softmax')\n",
    "        def call(self, inputs, training=False):\n",
    "            V_dst, V_dst_new = inputs\n",
    "            beta = self.f_beta(V_dst_new)\n",
    "            return beta*V_dst + (1-beta)*V_dst_new\n",
    "        \n",
    "    class f_v_up_alphabeta(tfkl.Layer):\n",
    "        def __init__(self, **kwargs):\n",
    "            super(f_v_up_beta, self).__init__(**kwargs)\n",
    "            self.f_beta = tfkl.Dense(1, 'softmax')\n",
    "            self.f_alpha = tfkl.Dense(1, 'softmax')\n",
    "        def call(self, inputs, training=False):\n",
    "            V_dst, V_dst_new = inputs\n",
    "            alpha = self.f_alpha(V_dst)\n",
    "            beta = self.f_beta(V_dst_new)\n",
    "            return alpha*V_dst + beta*V_dst_new\n",
    "        \n",
    "    @staticmethod\n",
    "    def f_inp_concat():\n",
    "        return tfkl.Concatenate()\n",
    "    \n",
    "    @staticmethod\n",
    "    def f_inp_edges():\n",
    "        return tfkl.Lambda(lambda V_src_loc, E: E)\n",
    "    \n",
    "    @staticmethod\n",
    "    def f_inp_verts():\n",
    "        return tfkl.Lambda(lambda V_src_loc, E: V_src_loc)\n",
    "    \n",
    "    @staticmethod\n",
    "    def f_a_up():\n",
    "        def f(x):\n",
    "            y=tfkl.Dense(1, 'softmax')(x)\n",
    "            y=tf.squeeze(y)\n",
    "            \n",
    "        return tfkl.Lambda(lambda E: f(E))\n",
    "    \n",
    "    @staticmethod\n",
    "    def f_e_up_const():\n",
    "        return tfkl.Lambda(lambda V_src, V_dst, E: E)\n",
    "    \n",
    "    class f_e_up_dense(tfkl.Layer):\n",
    "        def __init__(self, d_E **kwargs):\n",
    "            super(f_v_up_beta, self).__init__(**kwargs)\n",
    "            self.f_E_new = tfkl.Dense(d_E, 'relu')\n",
    "        def call(self, inputs, training=False):\n",
    "            # V_src, V_dst, E = inputs\n",
    "            return = self.f_E_new(tfkl.concatenate(input))\n",
    "        \n",
    "    class f_e_up_dense_oneway(tfkl.Layer):\n",
    "        def __init__(self, d_E **kwargs):\n",
    "            super(f_v_up_beta, self).__init__(**kwargs)\n",
    "            self.f_E_new = tfkl.Dense(d_E, 'relu')\n",
    "        def call(self, inputs, training=False):\n",
    "            V_src, V_dst, E = inputs\n",
    "            return = self.f_E_new(tfkl.concatenate([V_src, E]))\n",
    "        \n",
    "    class f_e_up_beta(tfkl.Layer):\n",
    "        def __init__(self, d_E **kwargs):\n",
    "            super(f_v_up_beta, self).__init__(**kwargs)\n",
    "            self.f_beta = tfkl.Dense(1, 'softmax')\n",
    "            self.f_E_new = tfkl.Dense(d_E, 'relu')\n",
    "        def call(self, inputs, training=False):\n",
    "            V_src, V_dst, E = inputs\n",
    "            beta = self.f_beta(tfkl.concatenate([V_src, V_dst]))\n",
    "            E_new = self.f_E_new(tfkl.concatenate(V_src, E))\n",
    "            return beta*V_dst + (1-beta)*E_new\n",
    "        \n",
    "    class f_e_up_attn(tfkl.Layer):\n",
    "        def __init__(self, d_val, d_key=8, pre_layer_normalization=True, **kwargs):\n",
    "            \"\"\"\n",
    "            pre-LN (https://arxiv.org/abs/2004.08249)\n",
    "            \"\"\"\n",
    "            super(f_pool_attn, self).__init__(**kwargs)\n",
    "            \n",
    "            self.pre_layer_normalization = pre_layer_normalization\n",
    "            if self.pre_layer_normalization:\n",
    "                self.V_dst_LN = tfkl.LayerNormalization()\n",
    "                self.inp_LN = tfkl.LayerNormalization()\n",
    "                \n",
    "            self.d_val = d_val\n",
    "            self.d_key = d_key\n",
    "            self.f_val = tfkl.Dense(d_val, 'relu')\n",
    "            self.f_key = tfkl.Dense(d_key, 'relu')\n",
    "            self.f_query = tfkl.Dense(d_key, 'relu')\n",
    "        def call(self, inputs, training=False):\n",
    "            # unpacking\n",
    "            V_src, V_dst, E = inputs\n",
    "\n",
    "            # pre-LN\n",
    "            if pre_layer_normalization:\n",
    "                V_dst = self.V_dst_LN(V_dst)\n",
    "                inp = self.inp_LN(inp)\n",
    "            \n",
    "            # generate queries, keys, and values\n",
    "            queries = self.f_query(V_dst)  # [..., N, d_key]\n",
    "            keys = self.f_key(inp) # [..., N, d_key]\n",
    "            vals = self.f_val(inp) # [..., N, d_val]\n",
    "            \n",
    "            # attention\n",
    "            score = tf.einsum('...qx,...kx->...qk', queries, keys)\n",
    "            score = score / tf.sqrt(self.d_key)\n",
    "            score = tf.nn.softmax(score, axis=-1)\n",
    "            \n",
    "            # weighted sum\n",
    "            pooled = tf.einsum('...qv,...vx->...qx', score, vals)\n",
    "            \n",
    "            return pooled            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
