{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tfk = tf.keras\n",
    "tfkl = tfk.layers\n",
    "\n",
    "#import tensorflow_probability as tfp\n",
    "#tfd = tfp.distributions\n",
    "#tfpl = tfp.layers\n",
    "#tfb = tfp.bijectors\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class GraphLayer(tfk.Model):\n",
    "    \n",
    "    def __init__(self,\n",
    "        f_inp,\n",
    "        f_pool,\n",
    "        f_v_up,\n",
    "        f_e_up,\n",
    "        f_adj_up,\n",
    "        **kwargs):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        \n",
    "        super(GraphLayer, self).__init__(kwargs)\n",
    "        \n",
    "        self.f_inp = f_inp\n",
    "        self.f_pool = f_pool\n",
    "        self.f_v_up = f_v_up\n",
    "        self.f_e_up = f_e_up\n",
    "        self.f_adj_up = f_adj_up\n",
    "        \n",
    "        self.f_V_src_loc = tfkl.Lambda(lambda V_src, A: \n",
    "            tf.einsum('...sv,...sd->...sdv', V_src, A))\n",
    "        self.f_V_dst_loc = tfkl.Lambda(lambda V_dst, A: \n",
    "            tf.einsum('...dv,...sd->...dsv', V_dst, A))\n",
    "        self.f_perm = tfkl.Lambda(lambda x: tf.einsum('...sdv->...dsv', x))\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        \n",
    "        # unpack inputs\n",
    "        V_src, V_dst, E, A = inputs\n",
    "        \n",
    "        # provide vert-localized copies of src and dst verts\n",
    "        V_src_loc = self.f_V_src_loc(V_src, A)\n",
    "        V_dst_loc = self.f_V_dst_loc(V_dst, A)\n",
    "        \n",
    "        # get src-dst pair-specific inputs to dst verts\n",
    "        inp = self.f_inp([V_src_loc, E], training=training)\n",
    "        inp = self.f_perm(inp)\n",
    "        \n",
    "        # pool src-dst pair-specific inputs\n",
    "        V_dst_new = self.f_pool([V_dst, inp], training=training)    \n",
    "        \n",
    "        # update dst verts\n",
    "        V_dst = self.f_v_up([V_dst, V_dst_new], training=training)\n",
    "        \n",
    "        # update edges\n",
    "        E = self.f_e_up([V_src_loc, V_dst_loc, E], training=training)\n",
    "        \n",
    "        # update adjacency matrix\n",
    "        A = self.f_adj_up(E, training=training)\n",
    "        \n",
    "        return V_dst, E, A\n",
    "    \n",
    "        \n",
    "    @staticmethod\n",
    "    def f_pool_sum():\n",
    "        return tfkl.Lambda(lambda V_dst, inp: tf.reduce_sum(inp, axis=-2))\n",
    "    @staticmethod\n",
    "    def f_pool_ave():\n",
    "        return tfkl.Lambda(lambda V_dst, inp: tf.reduce_mean(inp, axis=-2))\n",
    "    @staticmethod\n",
    "    def f_pool_prod():\n",
    "        return tfkl.Lambda(lambda V_dst, inp: tf.reduce_prod(inp, axis=-2))\n",
    "\n",
    "    class f_pool_attn(tfkl.Layer):\n",
    "        \n",
    "        def __init__(self, d_key=8, d_val=None, N_heads=8, pre_layer_normalization=True, **kwargs):\n",
    "            \"\"\"\n",
    "            pre-LN (https://arxiv.org/abs/2004.08249)\n",
    "            \"\"\"\n",
    "            super(GraphLayer.f_pool_attn, self).__init__(**kwargs)\n",
    "            \n",
    "            self.pre_layer_normalization = pre_layer_normalization                \n",
    "            self.d_key = d_key\n",
    "            self.d_val = self.d_key if d_val is None else d_val\n",
    "            self.N_heads = N_heads\n",
    "            \n",
    "            if self.pre_layer_normalization:\n",
    "                self.V_dst_LN = tfkl.LayerNormalization()\n",
    "                self.inp_LN = tfkl.LayerNormalization()\n",
    "            \n",
    "        def build(self, input_shape):\n",
    "            V_dst_shape, inp_shape = input_shape\n",
    "            \n",
    "            self.f_val = tfkl.Dense(self.N_heads * self.d_val, 'relu')\n",
    "            self.f_key = tfkl.Dense(self.N_heads * self.d_key, 'relu')\n",
    "            self.f_query = tfkl.Dense(self.N_heads * self.d_key, 'relu')\n",
    "            \n",
    "            self.reshape_q = tfkl.Reshape(V_dst_shape[:-2] +\n",
    "                (self.N_heads, self.d_key))\n",
    "            self.reshape_k = tfkl.Reshape(inp_shape[:-2] +\n",
    "                (self.N_heads, self.d_key))\n",
    "            self.reshape_v = tfkl.Reshape(inp_shape[:-2] +\n",
    "                (self.N_heads, self.d_val))\n",
    "            \n",
    "            def _f_MHA(queries, keys, values):\n",
    "                score = tf.einsum('...dhq,...dshq->dsh', queries, keys)\n",
    "                score = score / tf.sqrt(self.d_key)\n",
    "                score = tf.nn.softmax(score, axis=-1)\n",
    "                return tf.einsum('...dsh,...dshv->...dhv', score, values)\n",
    "            self.f_MHA = tfkl.Lambda(lambda q,k,v: _f_MHA(q,k,v))\n",
    "            \n",
    "            self.f_cat = tfkl.Reshape(V_dst_shape[:-1]+(-1,))\n",
    "            self.f_emb_cat = tfkl.Dense(V_dst_shape[-1], 'relu')\n",
    "        \n",
    "        def call(self, inputs, training=False):\n",
    "            # unpack inputs\n",
    "            V_dst, inp = inputs\n",
    "\n",
    "            # pre-LN\n",
    "            if self.pre_layer_normalization:\n",
    "                V_dst = self.V_dst_LN(V_dst, training=training)\n",
    "                inp = self.inp_LN(inp, training=training)\n",
    "            \n",
    "            # generate queries, keys, and values for all heads\n",
    "            queries = self.f_query(V_dst, training=training)  # [..., N_dst, N_heads*d_key]\n",
    "            keys = self.f_key(inp, training=training) # [..., N_dst, N_src, N_heads*d_key]\n",
    "            values = self.f_val(inp, training=training) # [..., N_dst, N_src, N_heads*d_val]\n",
    "            \n",
    "            # reshape into separate heads\n",
    "            queries = self.reshape_q(queries) # [..., N_dst, N_heads, d_key]\n",
    "            keys = self.reshape_k(keys) # [..., N_dst, N_heads, d_key]\n",
    "            values = self.reshape_v(values) # [..., N_dst, N_heads, d_key]\n",
    "            \n",
    "            # perform multi-head attention\n",
    "            mha_lookup = self.f_MHA([queries, keys, values], training=training)\n",
    "            # [..., N_dst, N_heads, d_val]\n",
    "            \n",
    "            # concatenate heads\n",
    "            mha_cat = self.f_cat(mha_lookup, training=training)\n",
    "            \n",
    "            # embed in output space\n",
    "            return self.f_emb_cat(mha_cat, training=training)\n",
    "\n",
    "    @staticmethod\n",
    "    def f_v_up_add():\n",
    "        return tfkl.Add()\n",
    "\n",
    "    @staticmethod\n",
    "    def f_v_up_direct():\n",
    "        return tfkl.Lambda(lambda V_dst, V_dst_new: V_dst_new)\n",
    "\n",
    "    class f_v_up_beta(tfkl.Layer):\n",
    "        def __init__(self, **kwargs):\n",
    "            super(GraphLayer.f_v_up_beta, self).__init__(**kwargs)\n",
    "            self.f_beta = tfkl.Dense(1, 'softmax')\n",
    "        def call(self, inputs, training=False):\n",
    "            V_dst, V_dst_new = inputs\n",
    "            beta = self.f_beta(V_dst_new)\n",
    "            return beta*V_dst + (1-beta)*V_dst_new\n",
    "        \n",
    "    class f_v_up_alphabeta(tfkl.Layer):\n",
    "        def __init__(self, **kwargs):\n",
    "            super(GraphLayer.f_v_up_alphabeta, self).__init__(**kwargs)\n",
    "            self.f_beta = tfkl.Dense(1, 'softmax')\n",
    "            self.f_alpha = tfkl.Dense(1, 'softmax')\n",
    "        def call(self, inputs, training=False):\n",
    "            V_dst, V_dst_new = inputs\n",
    "            alpha = self.f_alpha(V_dst)\n",
    "            beta = self.f_beta(V_dst_new)\n",
    "            return alpha*V_dst + beta*V_dst_new\n",
    "\n",
    "    @staticmethod\n",
    "    def f_inp_concat():\n",
    "        return tfkl.Concatenate()\n",
    "\n",
    "    @staticmethod\n",
    "    def f_inp_edges():\n",
    "        return tfkl.Lambda(lambda V_src_loc, E: E)\n",
    "\n",
    "    @staticmethod\n",
    "    def f_inp_verts():\n",
    "        return tfkl.Lambda(lambda V_src_loc, E: V_src_loc)\n",
    "\n",
    "    @staticmethod\n",
    "    def _f_adj_up():\n",
    "        def f(x):\n",
    "            y=tfkl.Dense(1, 'softmax')(x)\n",
    "            y=tf.squeeze(y)\n",
    "            \n",
    "        return tfkl.Lambda(lambda E: f(E))\n",
    "\n",
    "    @staticmethod\n",
    "    def f_e_up_const():\n",
    "        return tfkl.Lambda(lambda V_src_loc, V_dst_loc, E: E)\n",
    "\n",
    "    class f_e_up_dense(tfkl.Layer):\n",
    "        def __init__(self, **kwargs):\n",
    "            super(GraphLayer.f_e_up_dense, self).__init__(**kwargs)\n",
    "        def build(self, input_shape):\n",
    "            V_src_loc_shape, V_dst_loc_shape, E_shape = input_shape\n",
    "            self.f_E_new = tfkl.Dense(tf.shape(E_shape)[-1], 'relu')\n",
    "            self.V_dst_perm = tfkl.Lambda(\n",
    "                lambda x: tf.einsum('...dsv->...sdv', x))\n",
    "        def call(self, inputs, training=False):\n",
    "            V_src_loc, V_dst_loc, E = inputs\n",
    "            V_dst_loc_perm = self.V_dst_perm(V_dst_loc)\n",
    "            return self.f_E_new(tfkl.concatenate([\n",
    "                V_src_loc, V_dst_loc_perm, E]))\n",
    "        \n",
    "    class f_e_up_dense_oneway(tfkl.Layer):\n",
    "        def __init__(self, **kwargs):\n",
    "            super(GraphLayer.f_e_up_dense_oneway, self).__init__(**kwargs)\n",
    "        def build(self, input_shape):\n",
    "            V_src_loc_shape, V_dst_loc_shape, E_shape = input_shape\n",
    "            self.V_dst_perm = tfkl.Lambda(\n",
    "                lambda x: tf.einsum('...dsv->...sdv', x))\n",
    "            self.f_E_new = tfkl.Dense(tf.shape(E_shape)[-1], 'relu')\n",
    "        def call(self, inputs, training=False):\n",
    "            V_src_loc, V_dst_loc, E = inputs\n",
    "            return self.f_E_new(tfkl.concatenate([V_src_loc, E]))\n",
    "        \n",
    "    class f_e_up_beta(tfkl.Layer):\n",
    "        def __init__(self, **kwargs):\n",
    "            super(GraphLayer.f_e_up_beta, self).__init__(**kwargs)\n",
    "        def build(self, input_shape):\n",
    "            V_src_loc_shape, V_dst_loc_shape, E_shape = input_shape\n",
    "            self.V_dst_perm = tfkl.Lambda(\n",
    "                lambda x: tf.einsum('...dsv->...sdv', x))\n",
    "            self.f_beta = tfkl.Dense(1, 'softmax')\n",
    "            self.f_E_new = tfkl.Dense(tf.shape(E_shape)[-1], 'relu')\n",
    "        def call(self, inputs, training=False):\n",
    "            V_src_loc, V_dst_loc, E = inputs\n",
    "            V_dst_loc_perm = self.V_dst_perm(V_dst_loc)\n",
    "            E_new = self.f_E_new(tfkl.concatenate([\n",
    "                V_src_loc, V_dst_loc_perm, E]))\n",
    "            beta = self.f_beta(tfkl.concatenate([V_src_loc, V_dst_loc_perm]))\n",
    "            return beta*V_dst_loc_perm + (1-beta)*E_new\n",
    "\n",
    "    class f_e_up_attn(tfkl.Layer):\n",
    "        \n",
    "        def __init__(self, d_key=8, d_val=None, N_heads=8, pre_layer_normalization=True, **kwargs):\n",
    "            \"\"\"\n",
    "            pre-LN (https://arxiv.org/abs/2004.08249)\n",
    "            \"\"\"\n",
    "            super(GraphLayer.f_e_up_attn, self).__init__(**kwargs)\n",
    "            \n",
    "            self.pre_layer_normalization = pre_layer_normalization                \n",
    "            self.d_key = d_key\n",
    "            self.d_val = self.d_key if d_val is None else d_val\n",
    "            self.N_heads = N_heads\n",
    "            \n",
    "            if self.pre_layer_normalization:\n",
    "                self.V_dst_LN = tfkl.LayerNormalization()\n",
    "                self.inp_LN = tfkl.LayerNormalization()\n",
    "            \n",
    "        def build(self, input_shape):\n",
    "            V_src_loc_shape, V_dst_loc_shape, E_shape = input_shape\n",
    "            \n",
    "            self.V_dst_perm = tfkl.Lambda(\n",
    "                lambda x: tf.einsum('...dsv->...sdv', x))\n",
    "            \n",
    "            self.cat_q_data = tfkl.Concatenate()\n",
    "            self.cat_kv_data = tfkl.Concatenate()\n",
    "            \n",
    "            self.f_val = tfkl.Dense(self.N_heads * self.d_val, 'relu')\n",
    "            self.f_key = tfkl.Dense(self.N_heads * self.d_key, 'relu')\n",
    "            self.f_query = tfkl.Dense(self.N_heads * self.d_key, 'relu')\n",
    "            \n",
    "            self.reshape_q = tfkl.Reshape(E_shape[:-1] +\n",
    "                (self.N_heads, self.d_key))\n",
    "            self.reshape_k = tfkl.Reshape(E_shape[:-1] +\n",
    "                (self.N_heads, self.d_key))\n",
    "            self.reshape_v = tfkl.Reshape(E_shape[:-1] +\n",
    "                (self.N_heads, self.d_val))\n",
    "            \n",
    "            def _f_MHA(queries, keys, values):\n",
    "                score = tf.einsum('...sdhq,...sdhq->sdh', queries, keys)\n",
    "                score = score / tf.sqrt(self.d_key)\n",
    "                score = tf.nn.softmax(score, axis=-1)\n",
    "                return tf.einsum('...sdh,...sdhv->...dhv', score, values)\n",
    "            self.f_MHA = tfkl.Lambda(lambda q,k,v: _f_MHA(q,k,v))\n",
    "            \n",
    "            self.f_cat = tfkl.Reshape(E_shape[:-1]+(-1,))\n",
    "            self.f_emb_cat = tfkl.Dense(E_shape[-1], 'relu')\n",
    "        \n",
    "        def call(self, inputs, training=False):\n",
    "            # unpack inputs\n",
    "            V_src_loc, V_dst_loc, E = inputs\n",
    "\n",
    "            # pre-LN\n",
    "            if self.pre_layer_normalization:\n",
    "                V_dst = self.V_dst_LN(V_dst, training=training)\n",
    "                inp = self.inp_LN(inp, training=training)\n",
    "            \n",
    "            V_dst_loc_perm = self.V_dst_perm(V_dst_loc)\n",
    "            \n",
    "            q_data = self.cat_q_data([V_dst_loc, E])\n",
    "            kv_data = self.cat_kv_data([V_src_loc, E])\n",
    "            \n",
    "            # generate queries, keys, and values for all heads\n",
    "            queries = self.f_query(q_data, training=training)  # [..., N_src, N_dst, N_heads*d_key]\n",
    "            keys = self.f_key(kv_data, training=training) # [..., N_src, N_dst, N_heads*d_key]\n",
    "            values = self.f_val(kv_data, training=training) # [..., N_src, N_dst, N_heads*d_val]\n",
    "            \n",
    "            # reshape into separate heads\n",
    "            queries = self.reshape_q(queries) # [..., N_src, N_dst, N_heads, d_key]\n",
    "            keys = self.reshape_k(keys) # [..., N_src, N_dst, N_heads, d_key]\n",
    "            values = self.reshape_v(values) # [..., N_src, N_dst, N_heads, d_key]\n",
    "            \n",
    "            # perform multi-head attention\n",
    "            mha_lookup = self.f_MHA([queries, keys, values], training=training)\n",
    "            # [..., N_src, N_dst, N_heads, d_val]\n",
    "            \n",
    "            # concatenate heads\n",
    "            mha_cat = self.f_cat(mha_lookup, training=training)\n",
    "            # [..., N_src, N_dst, N_heads*d_val]\n",
    "            \n",
    "            # embed in output space\n",
    "            return self.f_emb_cat(mha_cat, training=training)\n",
    "            # [..., N_src, N_dst, d_E]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gn1=GraphLayer(\n",
    "    f_inp=GraphLayer.f_inp_concat,\n",
    "    f_pool=GraphLayer.f_pool_attn(\n",
    "        d_key=8, d_val=16, N_heads=8),\n",
    "    f_v_up=GraphLayer.f_v_up_beta(),\n",
    "    f_e_up=GraphLayer.f_e_up_dense(),\n",
    "    f_adj_up=GraphLayer._f_adj_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "little_g = (\n",
    "    tf.ones((9,5)),\n",
    "    tf.ones((8,4)),\n",
    "    tf.ones((9,8,10)),\n",
    "    tf.ones((9,8))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gn1.call(little_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Multigraph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-ea90092276de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m mg = Multigraph(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mVs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"cell\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mEs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"cell\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     As={\"cell\":tf.zeros((N_v, N_v))})\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Multigraph' is not defined"
     ]
    }
   ],
   "source": [
    "mg = Multigraph(\n",
    "    Vs={\"cell\":tf.zeros((N_v, d_v))},\n",
    "    Es={\"cell\":tf.zeros((N_v, N_v, d_v))},\n",
    "    As={\"cell\":tf.zeros((N_v, N_v))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mg.add_root_network(\n",
    "    root_name=\"in\",\n",
    "    intragraph_density=0.0,\n",
    "    intergraph_density=1.0,\n",
    "    neighbors=[\"cell\"],\n",
    "    connection_direction=[\"src\"],\n",
    "    N_v=1,\n",
    "    d_v=d_in)\n",
    "mg.add_root_network(\n",
    "    root_name=\"out\",\n",
    "    intragraph_density=0.0,\n",
    "    intergraph_density=1.0,\n",
    "    neighbors=[\"cell\"],\n",
    "    connection_direction=[\"dst\"],\n",
    "    N_v=1,\n",
    "    d_v=d_out)\n",
    "\n",
    "rnn = tfkl.RNN(MultiGraphRNNCell(\n",
    "    multigraph_template=mg,\n",
    "    f_rel_update={\n",
    "        # the input layer\n",
    "        (\"in\", \"cell\"): GraphLayer(\n",
    "            f_inp=GraphLayer.f_inp_concat,\n",
    "            f_pool=GraphLayer.f_pool_attn(\n",
    "                d_key=8, d_val=16, N_heads=8),\n",
    "            f_v_up=GraphLayer.f_v_up_beta(),\n",
    "            f_e_up=GraphLayer.f_e_up_dense(),\n",
    "            f_adj_up=GraphLayer._f_adj_up),\n",
    "        # the working memory layer\n",
    "        (\"cell\", \"cell\"): GraphLayer(\n",
    "            f_inp=GraphLayer.f_inp_concat,\n",
    "            f_pool=GraphLayer.f_pool_attn(\n",
    "                d_key=16, d_val=64, N_heads=8),\n",
    "            f_v_up=GraphLayer.f_v_up_beta(),\n",
    "            f_e_up=GraphLayer.f_e_up_attn(\n",
    "                d_key=8, d_val=16, N_heads=8),\n",
    "            f_adj_up=GraphLayer._f_adj_up),\n",
    "        # the output layer\n",
    "        (\"cell\", \"out\"): GraphLayer(\n",
    "            f_inp=GraphLayer.f_inp_concat,\n",
    "            f_pool=GraphLayer.f_pool_attn(\n",
    "                d_key=8, d_val=16, N_heads=8),\n",
    "            f_v_up=GraphLayer.f_v_up_beta(),\n",
    "            f_e_up=GraphLayer.f_e_up_attn(\n",
    "                d_key=4, d_val=8, N_heads=8),\n",
    "            f_adj_up=GraphLayer._f_adj_up)},\n",
    "    f_inp=MultiGraphRNNCell.f_inp_update_root(\"in\"),\n",
    "    f_update_seq=(lambda x: [\n",
    "        (\"in\", \"cell\"),\n",
    "        (\"cell\", \"cell\"),\n",
    "        (\"cell\", \"out\")]),\n",
    "    f_ret=MultiGraphRNNCell.f_ret_just_root(\"out\")))\n",
    "return rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class Multigraph:\n",
    "    \n",
    "    def __init__(self, Vs, Es, As):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.Vs = Vs\n",
    "        self.Es = Es\n",
    "        self.As = As\n",
    "    \n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            \"Vs\": self.Vs,\n",
    "            \"Es\": self.Es,\n",
    "            \"As\": self.As\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_dict(dict):\n",
    "        return Multigraph(\n",
    "            Vs=dict[\"Vs\"],\n",
    "            Es=dict[\"Es\"],\n",
    "            As=dict[\"As\"])\n",
    "\n",
    "    def to_list(self):\n",
    "        return [\n",
    "            self.Vs[k] for k in list(self.Vs.keys())\n",
    "        ] + [\n",
    "            self.Es[k] for k in list(self.Es.keys())\n",
    "        ] + [\n",
    "            self.As[k] for k in list(self.As.keys())\n",
    "        ]\n",
    "    \n",
    "    def load_from_list(self, list_inp):\n",
    "\n",
    "        i = 0\n",
    "\n",
    "        for k in list(self.Vs.keys()):\n",
    "            self.Vs[k] = list_inp[i]\n",
    "            i = i + 1\n",
    "\n",
    "        for k in list(self.Es.keys()):\n",
    "            self.Es[k] = list_inp[i]\n",
    "            i = i + 1\n",
    "\n",
    "        for k in list(self.As.keys()):\n",
    "            self.As[k] = list_inp[i]\n",
    "            i = i + 1\n",
    "    \n",
    "    @property\n",
    "    def N_v(self, name):\n",
    "        return tf.shape(self.Vs[name])[-2]\n",
    "    @property\n",
    "    def d_v(self, name):\n",
    "        return tf.shape(self.Vs[name])[-1]\n",
    "    @property\n",
    "    def d_e(self, src, dst):\n",
    "        return tf.shape(self.Es[(src, dst)])[-1]\n",
    "    \n",
    "    def connect_graphs(self, src, dst, e_emb=tf.ones((1,)), density=1.0):\n",
    "        leading_dims = tf.shape(self.Vs[src])[:-2]\n",
    "        N_src = tf.shape(self.Vs[src])[-2:-1]\n",
    "        N_dst = tf.shape(self.Vs[dst])[-2:-1]\n",
    "        self.As[(src,dst)] = tf.cast(tf.random.uniform(\n",
    "            tf.concat([leading_dims, N_src, N_dst], axis=0)) < density,\n",
    "            tfk.backend.floatx())\n",
    "        self.Es[(src,dst)] = tf.einsum('...sd,v->...sdv',\n",
    "            self.As[(src,dst)], e_emb)\n",
    "    \n",
    "    def add_root_network(self,\n",
    "        root_name,\n",
    "        intragraph_density=1.0,\n",
    "        intergraph_density=1.0,\n",
    "        neighbors=[],\n",
    "        connection_direction=[\"src\", \"dst\"],\n",
    "        N_v=1,\n",
    "        d_v=1):\n",
    "        \"\"\"convenience function to make root node network\n",
    "        and connect to other graphs. Root networks provide an\n",
    "        information highway for intragraph vert updates and\n",
    "        can be used to connect heterogenous graphs.\n",
    "        \n",
    "        WARNING: the multigraph must have at least one other\n",
    "        set of verts so we can detirmine batch size and time\n",
    "        steps (or any other leading dimensions).\n",
    "        \n",
    "        neighbors (list<str>): neighboring graphs (if any) to\n",
    "            connect new root network to. \"src\" means the root\n",
    "            is a source and the neighbors are destination graphs.\n",
    "            \"dst\" means the root graph is an innode in the graph\n",
    "            network. Both directions can be specified.\n",
    "        \"\"\"\n",
    "        \n",
    "        # create root graph verts\n",
    "        leading_dims = tf.shape(list(self.Vs.values())[0])[:-2]\n",
    "        print(\"new shape: \", tf.concat([leading_dims,\n",
    "                tf.TensorShape((N_v, d_v))], axis=0))\n",
    "        self.Vs[root_name] = tf.zeros(tf.concat([leading_dims,\n",
    "                tf.TensorShape((N_v, d_v))], axis=0))\n",
    "        \n",
    "        # connect graph internally\n",
    "        if intragraph_density != 0:\n",
    "            self.connect_graphs(root_name, root_name, \n",
    "                density=intragraph_density)\n",
    "        \n",
    "        # connect with neighbors\n",
    "        for neighbor in neighbors:\n",
    "            if \"src\" in connection_direction:\n",
    "                self.connect_graphs(root_name, neighbor,\n",
    "                    density=intergraph_density)\n",
    "            if \"dst\" in connection_direction:\n",
    "                self.connect_graphs(neighbor, root_name,\n",
    "                    density=intergraph_density)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    mg = Multigraph(\n",
    "        Vs={\"cell\":tf.zeros((N_v, d_v))},\n",
    "        Es={\"cell\":tf.zeros((N_v, N_v, d_v))},\n",
    "        As={\"cell\":tf.zeros((N_v, N_v))})\n",
    "    mg.add_root_network(\n",
    "        root_name=\"in\",\n",
    "        intragraph_density=0.0,\n",
    "        intergraph_density=1.0,\n",
    "        neighbors=[\"cell\"],\n",
    "        connection_direction=[\"src\"],\n",
    "        N_v=1,\n",
    "        d_v=d_in)\n",
    "    mg.add_root_network(\n",
    "        root_name=\"out\",\n",
    "        intragraph_density=0.0,\n",
    "        intergraph_density=1.0,\n",
    "        neighbors=[\"cell\"],\n",
    "        connection_direction=[\"dst\"],\n",
    "        N_v=1,\n",
    "        d_v=d_out)\n",
    "    \n",
    "    rnn = tfkl.RNN(MultiGraphRNNCell(\n",
    "        multigraph_template=mg,\n",
    "        f_rel_update={\n",
    "            # the input layer\n",
    "            (\"in\", \"cell\"): GraphLayer(\n",
    "                f_inp=GraphLayer.f_inp_concat,\n",
    "                f_pool=GraphLayer.f_pool_attn(\n",
    "                    d_key=8, d_val=16, N_heads=8),\n",
    "                f_v_up=GraphLayer.f_v_up_beta(),\n",
    "                f_e_up=GraphLayer.f_e_up_dense(),\n",
    "                f_adj_up=GraphLayer._f_adj_up),\n",
    "            # the working memory layer\n",
    "            (\"cell\", \"cell\"): GraphLayer(\n",
    "                f_inp=GraphLayer.f_inp_concat,\n",
    "                f_pool=GraphLayer.f_pool_attn(\n",
    "                    d_key=16, d_val=64, N_heads=8),\n",
    "                f_v_up=GraphLayer.f_v_up_beta(),\n",
    "                f_e_up=GraphLayer.f_e_up_attn(\n",
    "                    d_key=8, d_val=16, N_heads=8),\n",
    "                f_adj_up=GraphLayer._f_adj_up),\n",
    "            # the output layer\n",
    "            (\"cell\", \"out\"): GraphLayer(\n",
    "                f_inp=GraphLayer.f_inp_concat,\n",
    "                f_pool=GraphLayer.f_pool_attn(\n",
    "                    d_key=8, d_val=16, N_heads=8),\n",
    "                f_v_up=GraphLayer.f_v_up_beta(),\n",
    "                f_e_up=GraphLayer.f_e_up_attn(\n",
    "                    d_key=4, d_val=8, N_heads=8),\n",
    "                f_adj_up=GraphLayer._f_adj_up)},\n",
    "        f_inp=MultiGraphRNNCell.f_inp_update_root(\"in\"),\n",
    "        f_update_seq=(lambda x: [\n",
    "            (\"in\", \"cell\"),\n",
    "            (\"cell\", \"cell\"),\n",
    "            (\"cell\", \"out\")]),\n",
    "        f_ret=MultiGraphRNNCell.f_ret_just_root(\"out\")))\n",
    "    return rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiGraphRNNCell(tfkl.Layer):\n",
    "    \n",
    "    def __init__(self,\n",
    "        multigraph_template,\n",
    "        f_rel_update,\n",
    "        f_inp=(lambda inp, mg: mg),\n",
    "        f_update_seq=None,\n",
    "        f_ret=(lambda x: x),\n",
    "        randomized_update_seq=False):\n",
    "        \"\"\"\n",
    "        f_rel_update (dict<(str,str): GraphLayer): update functions\n",
    "            for each source-destination graph pairs. If `None`, specify\n",
    "            an `f_rel_update_model` that will be applied to all edges in\n",
    "            the multigraph.\n",
    "        f_rel_update_model (GraphLayer): updating function to be copied\n",
    "            for all source-destination graph relations in the case that\n",
    "            `f_rel_update` is `None`.\n",
    "        \"\"\"\n",
    "        self.multigraph_template = multigraph_template\n",
    "        self.f_rel_update = f_rel_update\n",
    "        self.f_inp = f_inp\n",
    "        if f_update_seq is None:\n",
    "            f_update_seq = MultiGraphRNNCell.f_update_seq_egocentric\n",
    "        self.f_update_seq = f_update_seq\n",
    "        self.f_ret = f_ret\n",
    "        \n",
    "        ## RNNCell attributes\n",
    "        self.state_size = tf.shape(multigraph_template.to_list())\n",
    "        \n",
    "        f_ret_template = f_ret(multigraph_template)\n",
    "        if isinstance(f_ret_template, Multigraph):\n",
    "            f_ret_template = f_ret_template.to_dict()\n",
    "        self.output_size = tf.shape(f_ret_template)\n",
    "\n",
    "        self.randomized_update_seq = randomized_update_seq\n",
    "    \n",
    "    def call(self, input_at_t, state_at_t, training=False):\n",
    "        multigraph = self.multigraph_template.load_from_list(state_at_t)\n",
    "        multigraph = self.f_inp(input_at_t, multigraph)\n",
    "        for rel in self.f_update_seq(multigraph):\n",
    "            src, dst = rel\n",
    "            multigraph.V[dst], multigraph.E[rel], multigraph.A[rel] = \\\n",
    "                self.f_rel_update[rel](\n",
    "                    multigraph.V[src], multigraph.V[dst],\n",
    "                    multigraph.E[rel], multigraph.A[rel])\n",
    "        return self.f_ret(multigraph), multigraph.to_list()\n",
    "    \n",
    "    def get_initial_state(self, inputs=None, batch_size=None, dtype=None):\n",
    "        return self.multigraph_template.to_list()\n",
    "    \n",
    "    @staticmethod\n",
    "    class f_inp_update_root:\n",
    "        def __init__(self, root_name):\n",
    "            self.root_name = root_name\n",
    "        def __call__(self, inputs, multigraph):\n",
    "            multigraph.Vs[self.root_name][...,0,:]=inputs\n",
    "            return multigraph\n",
    "    \n",
    "    @staticmethod\n",
    "    def f_update_seq_reg(multigraph):\n",
    "        \"\"\"just go through all defined relations\"\"\"\n",
    "        seq = list(multigraph.Vs.keys())\n",
    "        if multigraph.randomized_update_seq:\n",
    "            random.shuffle(seq)\n",
    "        return seq\n",
    "    \n",
    "    @staticmethod\n",
    "    def f_update_seq_egocentric(multigraph):\n",
    "        \"\"\"first perform intragraph update, then intergraph update\"\"\"\n",
    "        \n",
    "        all_pairs = list(multigraph.Es.keys())\n",
    "        if multigraph.randomized_update_seq:\n",
    "            random.shuffle(all_pairs)\n",
    "            \n",
    "        intragraph_pairs = [(src,dst) for (src,dst)\n",
    "                            in all_pairs if src==dst]\n",
    "        intergraph_pairs = [(src,dst) for (src,dst)\n",
    "                            in all_pairs if src!=dst]\n",
    "                    \n",
    "        return intragraph_pairs + intergraph_pairs\n",
    "    \n",
    "    @staticmethod\n",
    "    class f_ret_just_graph:\n",
    "        def __init__(self, graph_name):\n",
    "            self.graph_name = graph_name\n",
    "        \n",
    "        def __call__(self, multigraph):\n",
    "            return (multigraph.Vs[self.graph_name],\n",
    "                    multigraph.Es[(self.graph_name, self.graph_name)],\n",
    "                    multigraph.As[(self.graph_name, self.graph_name)])\n",
    "        \n",
    "    @staticmethod\n",
    "    class f_ret_just_root:\n",
    "        def __init__(self, root_name):\n",
    "            self.root_name = root_name\n",
    "        \n",
    "        def __call__(self, multigraph):\n",
    "            return tf.reduce_mean(\n",
    "                multigraph.Vs[self.root_name],\n",
    "                axis=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dense2denseRNN(d_in, d_out, N_v=64, d_v=16, d_e=8):\n",
    "    \"\"\"Convenience initializer for MHA graph RNN with dense input and output\n",
    "    \"\"\"\n",
    "    \n",
    "    mg = Multigraph(\n",
    "        Vs={\"cell\":tf.zeros((N_v, d_v))},\n",
    "        Es={\"cell\":tf.zeros((N_v, N_v, d_v))},\n",
    "        As={\"cell\":tf.zeros((N_v, N_v))})\n",
    "    mg.add_root_network(\n",
    "        root_name=\"in\",\n",
    "        intragraph_density=0.0,\n",
    "        intergraph_density=1.0,\n",
    "        neighbors=[\"cell\"],\n",
    "        connection_direction=[\"src\"],\n",
    "        N_v=1,\n",
    "        d_v=d_in)\n",
    "    mg.add_root_network(\n",
    "        root_name=\"out\",\n",
    "        intragraph_density=0.0,\n",
    "        intergraph_density=1.0,\n",
    "        neighbors=[\"cell\"],\n",
    "        connection_direction=[\"dst\"],\n",
    "        N_v=1,\n",
    "        d_v=d_out)\n",
    "    \n",
    "    rnn = tfkl.RNN(MultiGraphRNNCell(\n",
    "        multigraph_template=mg,\n",
    "        f_rel_update={\n",
    "            # the input layer\n",
    "            (\"in\", \"cell\"): GraphLayer(\n",
    "                f_inp=GraphLayer.f_inp_concat,\n",
    "                f_pool=GraphLayer.f_pool_attn(\n",
    "                    d_key=8, d_val=16, N_heads=8),\n",
    "                f_v_up=GraphLayer.f_v_up_beta(),\n",
    "                f_e_up=GraphLayer.f_e_up_dense(),\n",
    "                f_adj_up=GraphLayer._f_adj_up),\n",
    "            # the working memory layer\n",
    "            (\"cell\", \"cell\"): GraphLayer(\n",
    "                f_inp=GraphLayer.f_inp_concat,\n",
    "                f_pool=GraphLayer.f_pool_attn(\n",
    "                    d_key=16, d_val=64, N_heads=8),\n",
    "                f_v_up=GraphLayer.f_v_up_beta(),\n",
    "                f_e_up=GraphLayer.f_e_up_attn(\n",
    "                    d_key=8, d_val=16, N_heads=8),\n",
    "                f_adj_up=GraphLayer._f_adj_up),\n",
    "            # the output layer\n",
    "            (\"cell\", \"out\"): GraphLayer(\n",
    "                f_inp=GraphLayer.f_inp_concat,\n",
    "                f_pool=GraphLayer.f_pool_attn(\n",
    "                    d_key=8, d_val=16, N_heads=8),\n",
    "                f_v_up=GraphLayer.f_v_up_beta(),\n",
    "                f_e_up=GraphLayer.f_e_up_attn(\n",
    "                    d_key=4, d_val=8, N_heads=8),\n",
    "                f_adj_up=GraphLayer._f_adj_up)},\n",
    "        f_inp=MultiGraphRNNCell.f_inp_update_root(\"in\"),\n",
    "        f_update_seq=(lambda x: [\n",
    "            (\"in\", \"cell\"),\n",
    "            (\"cell\", \"cell\"),\n",
    "            (\"cell\", \"out\")]),\n",
    "        f_ret=MultiGraphRNNCell.f_ret_just_root(\"out\")))\n",
    "    return rnn\n",
    "\n",
    "dense2denseRNN(d_in=16, d_out=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
