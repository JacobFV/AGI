{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "PAI_0.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "expired-dragon",
        "bSCj1QAVrFnC",
        "qADn6FLHWFLc",
        "PWqC-aSuxTSE"
      ],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JacobFV/AGI/blob/master/PAI_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stuffed-fancy"
      },
      "source": [
        "# PAI-0"
      ],
      "id": "stuffed-fancy"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Un405Z8LauWF"
      },
      "source": [
        "PAI-0 extracts and integrates principles from a forgotten array of AI and neuroscientific research notably including (in chronological order):\n",
        "- Buzsaki's [*Rhythms of the Brain*](https://psycnet.apa.org/record/2007-01020-000)\n",
        "- The Paradigm of Allostatic Orchestration ([Lee 2019](https://www.frontiersin.org/articles/10.3389/fnhum.2019.00129/full)).\n",
        "- LGMA architecture ([Qi 2020](https://arxiv.org/abs/2011.11400))\n",
        "- Hinton's [NSF presentation](https://www.hpcwire.com/off-the-wire/nsf-distinguished-lecture-with-geoffrey-hinton-how-to-represent-part-whole-hierarchies-in-a-neural-net-to-be-held-feb-11/) on GLOM\n",
        "- Begg's introduction to the critical neuron hypothesis ([Beggs 2015](https://youtu.be/bE9IKMAr-wg))\n",
        "- The SORN design ([Lazar et al. 2009](https://www.frontiersin.org/articles/10.3389/neuro.10.023.2009/full))\n",
        "- Buzsaki's [*The Brain from Inside Out*](https://buzsakilab.com/wp/2019/02/06/the-brain-from-inside-out-by-buzsaki-g/)"
      ],
      "id": "Un405Z8LauWF"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-03-29T17:04:26.254500Z",
          "iopub.status.busy": "2021-03-29T17:04:26.254187Z",
          "iopub.status.idle": "2021-03-29T17:04:44.716925Z",
          "shell.execute_reply": "2021-03-29T17:04:44.716390Z",
          "shell.execute_reply.started": "2021-03-29T17:04:26.254425Z"
        },
        "tags": [],
        "id": "vocational-commissioner",
        "cellView": "form"
      },
      "source": [
        "#@title imports\n",
        "%tensorflow_version 2.x\n",
        "\n",
        "import math\n",
        "import tqdm\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "!pip install -q tsalib\n",
        "import tsalib\n",
        "import networkx\n",
        "!pip install -q jplotlib\n",
        "import jplotlib as jpl\n",
        "!pip install -q livelossplot\n",
        "from livelossplot import PlotLossesKeras\n",
        "\n",
        "import tensorflow as tf\n",
        "keras = tf.keras\n",
        "tfkl = keras.layers\n",
        "\n",
        "import tensorflow_probability as tfp\n",
        "tfd = tfp.distributions\n",
        "tfpl = tfp.layers\n",
        "tfb = tfp.bijectors"
      ],
      "id": "vocational-commissioner",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yOiSVPS6jXY",
        "cellView": "form"
      },
      "source": [
        "#@title utils\n",
        "\n",
        "def Bernoulli(p, shape=None):\n",
        "    return tf.cast(tf.random.uniform(shape) < p, tf.float32)\n",
        "\n",
        "def clear_diag(A):\n",
        "    return A * (1 - tf.eye(tf.shape(A)[-2], tf.shape(A)[-1]))\n",
        "\n",
        "def TODO(reason=\"\"):\n",
        "    raise NotImplementedError(\"TODO: \" + reason)"
      ],
      "id": "0yOiSVPS6jXY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhCXWu8LspxA"
      },
      "source": [
        "## Local Dynamics"
      ],
      "id": "FhCXWu8LspxA"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86WLqIC2slSn"
      },
      "source": [
        "## Global Dynamics"
      ],
      "id": "86WLqIC2slSn"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f_gIHf-gRt6"
      },
      "source": [
        "I extract a few **hypothetical** principles from the reward system's natural implementation to apply in SSORN:\n",
        "\n",
        "- **Reward is not a scalar or directly controllable signal.** The reward *system* is not one neuron. The brain develops multiple non-uniformly distributed dopaminergic pathways. Affective psychology commonly divides affective state into arousal (awareness), valency (positive/negative), and motivational intensity (how disposed to act). These physchological variables are not directly reachable over significant timescales; they emerge from the complex and chaotic interactions associated with neurotransmitters, brain-scale patterns, the body's internal environment, and external stimuli and actions. **Implication**: Use multiple layers and connections to propagate reward. Also divide some of the reward system into positive and negative valency processing subsystems respectively. Include a joint reward subsystem that integrates with both these subsystems and the overall network.\n",
        "\n",
        "- **Reward may be intrinsic, conditioned, or even imagined** Not just external behavior -- but even the reward system itself -- shared similar responsive and adaptive mechanisms as other parts of the brain do. **Implication**: The reward system layers and connections should share similar activity and learning algorithms as most other layers and connections. The reward system should also recieve its input from the rest of the brain.\n",
        "\n",
        "- **Reward biases neural activity nonhomeogeneously** Action pathways  \n",
        "\n",
        "- **The global reward objective generally aligns with local neuronal objectives** The free energy principles suggests that the brain's objective is to accurately model its external environment. The neuronal energy homeostasis principle suggests that locally excessive or lacking neuronal energy stressors drives adaptations toward network-scale behavior. **Implication**: Supposing neurons represent binary random variables, their average activation should not be close to 0 (representing a negative energy stressor) nor should it be too close to 1 (representing positive a energy stressor). The reward system's activity will increase when mean neuronal activation deviates from both 0 and 1 by minimizing the objective $D_{KL}(p(x)||\\mathcal{B}(x; p=0.2))$\n",
        "\n",
        "- **The reward system is robust to tampering** Returning to the neuronal energy homeostasis principle, an excessive activitiy poses an energy stressor which neurons adapt to by decreasing the number of presynaptic inputs, increasing their resistivity to signal propagation, and raising the bar for action potentiation. This makes reward an oscilatory rather than constant experience and often tunes human behavior to a safe optima of drinking/eating frequency. **Implication** While the network's average activity deviation from 0 and 1 regularizes the reward system as a whole, internal reward system neurons will also be individually regularized by their own average activity. This should make eliciting reward a critically balanced problem.\n",
        "\n",
        "- **Anticapated reward is chaotic and discovers complex and diverse behaviors** The brain's anticipation of affective state is often incorrect for any significant distance in time. Following this seemingly chaotic signal helps 'shake' natural agents outside local optima and at times display behavior that contradicts externally administered reward. **Implication** The reward system should not be a strong contributor to presynaptic signal accumulation. It primary influence should be on neuronal activity thresholds and noise which may compare to shifting the temperaterature parameter of an Ising model to be subcritical. \n",
        "\n",
        "- **The reward system operates over microscopic and macroscopic scales with fast and slow durations**. Principles and tools from relativity apply well to analyzing and modeling the brain's spatiotemporal activity. **Implications**:\n",
        "    - **Fast and small**: Decrease the temperature coefficent and threshold required for excitatory and inhibitory signal propagation. Make inhibitory signal threasholds slightly lower than for excitatory such that the total number of signals propagating through the SSORN slightly decreases. Inhibitory thresholds should be even lower when a consequence of negative as opposed to positive reward subsystem activity.\n",
        "    - **Fast and large**: By lowering neuronal activation threshold, the reward is expected to decrease the signal to noise ratio when fast behavioral response is needed. It should also lower the probability of exploratory dynamics in response to intense positive and especially negative situations.\n",
        "    - **Slow and small**: Learning rates across the network should be porpotional to the rolling mean of reward system activity. \n",
        "    - **Slow and large**: The 'reward system' (used here to describe the layers and connections explicitly labeled as such) makes little distinction between positively or negatively valent network states which themselves may or may not be quarrelated to desirable or undesirable behaviors. Actually learning desirable behavior is the unsupervised, free-energy minimizing (e.g.: diverse, predictive, predictable, empowering, etc.. In biological systems, physiologically optimal) objective which should implicitly emerge from the integrated dynamics of PAI-0. The agent should eventually use its associated phenomonal experience of reward to form explicit associations with significantly faster semantic learning."
      ],
      "id": "0f_gIHf-gRt6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXK-wO2YYNRx"
      },
      "source": [
        "The question is still: how do you bias network activity towards 'desirable' states?\n",
        " - $\\min f(x) = -x(x-1)$ or similar makes the network *able* to learn\n",
        " - plasticity should increase approaching either extreme\n",
        " - new synapses that were used should be strengthened (weakened) by positive (negative) reward; synapses that were not used should not be modified. Whatever rule to express this should also account for IE/EI/EE/II against +/- reward.\n"
      ],
      "id": "qXK-wO2YYNRx"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtQW2cJLfu0C"
      },
      "source": [
        "class Updatable:\n",
        "    \n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "\n",
        "    def update(self, state): pass # returns updated state\n",
        "\n",
        "    @property\n",
        "    def initial_state(self): pass # gets a (optionally nested) tensor"
      ],
      "id": "TtQW2cJLfu0C",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROmi23tkZGFy"
      },
      "source": [
        "class Layer(Updatable):\n",
        "\n",
        "    def __init__(self,\n",
        "        height,\n",
        "        width,\n",
        "        depth,\n",
        "        initial_threashold,\n",
        "        initial_bucket_val,\n",
        "        noise_fn,\n",
        "        activation_penalty,\n",
        "        target_firing_rate,\n",
        "        target_firing_rate_lr,\n",
        "        name,\n",
        "    ):\n",
        "        pass\n",
        "    \n",
        "    def update(self, state): pass # returns updated state\n",
        "\n",
        "    @property\n",
        "    def initial_state(self): pass # gets a (optionally nested) tensor"
      ],
      "id": "ROmi23tkZGFy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIKf7hI8YtuV"
      },
      "source": [
        "class Connection(Updatable):\n",
        "\n",
        "    def __init__(\n",
        "        self, \n",
        "        src_layer,\n",
        "        dst_layer,\n",
        "        src_sparsity,\n",
        "        dst_sparsity,\n",
        "        threshold_coef,\n",
        "        bucket_coef,\n",
        "        name\n",
        "    ):\n",
        "        pass\n",
        "\n",
        "    def update(self, state):\n",
        "        state = self.fast_action(state)\n",
        "        state = self.slow_action(state)\n",
        "        return state\n",
        "\n",
        "    def _fast_action(state):\n",
        "        # same for (E/I)-(E/I)Layers\n",
        "\n",
        "    def _slow_action(state):\n",
        "        raise NotImplemntedError(\"subclasses should implement this\")\n",
        "\n",
        "    @property\n",
        "    def initial_state(self): pass # gets a (optionally nested) tensor"
      ],
      "id": "pIKf7hI8YtuV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIlteZ8Waqps"
      },
      "source": [
        "class LateralConnection(Connection):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        connectivity_falloff, # a-value in f(d) = d^a for semi-global connectivity\n",
        "        threshold_coef,\n",
        "        bucket_coef,\n",
        "    )\n",
        "\n",
        "    def _slow_action(state):\n",
        "        raise NotImplemntedError(\"subclasses should implement this\")\n",
        "\n",
        "    @property\n",
        "    def initial_state(self): pass # gets a (optionally nested) tensor"
      ],
      "id": "sIlteZ8Waqps",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_Llw7wKar8U"
      },
      "source": [
        "class EEConnection(Connection): pass\n",
        "class EIConnection(Connection): pass\n",
        "class IEConnection(Connection): pass\n",
        "class IIConnection(Connection): pass"
      ],
      "id": "H_Llw7wKar8U",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yW_23Hn1dMW3"
      },
      "source": [
        "class MotorNerve(Updatable): \n",
        "\n",
        "    def __init__(self, src, connection_sparsity, dst_key): \n",
        "        TODO()"
      ],
      "id": "yW_23Hn1dMW3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxeC-s8tdVrY"
      },
      "source": [
        "class SensoryNerve(Updatale): \n",
        "\n",
        "    def __init__(self, )"
      ],
      "id": "yxeC-s8tdVrY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NK_xLR2-bHEE"
      },
      "source": [
        "class PAI0:\n",
        "\n",
        "    def __init__(self, connections=None, updatables=None):\n",
        "        # please specify at least one param\n",
        "\n",
        "        # get unique updatables\n",
        "        self.updatables = updatables if updatables is not None else []\n",
        "\n",
        "        # add layers and connections\n",
        "        _layers_and_connections = list()\n",
        "        for src_layer, connection, dst_layer in connections:\n",
        "            _layers_and_connections += [src_layer, connection dst_layer]\n",
        "        #  avoids double counting\n",
        "        self.updatables += list(set(_layers_and_connections) - set(self.updatables))\n",
        "\n",
        "\n",
        "    @tf.function\n",
        "    def update(self, prev_state):\n",
        "\n",
        "        return state\n",
        "\n",
        "    @property\n",
        "    def initial_state(self): pass # gets a (optionally nested) tensor\n",
        "\n",
        "    \"\"\"@property\n",
        "    def get_layers():\n",
        "        return [updatable for updatable \n",
        "                in self.updatables \n",
        "                if isinstance(updatable, Layer)]\n",
        "    @property\n",
        "    def get_connections():\n",
        "        return [updatable for updatable \n",
        "                in self.updatables \n",
        "                if isinstance(updatable, Layer)]\"\"\""
      ],
      "id": "NK_xLR2-bHEE",
      "execution_count": null,
      "outputs": []
    }
  ]
}